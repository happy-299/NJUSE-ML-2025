{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 数据预处理\n",
    "使用了所有提供的数据集，共10个。"
   ],
   "id": "b61fd27a39e74482"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 完成解压\n",
   "id": "6a3bf1841ed221de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 创建dataset目录（如果不存在）\n",
    "dataset_dir = 'dataset'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# 原始数据集路径\n",
    "original_dir = 'original_dataset'\n",
    "\n",
    "print(\"开始解压并整理数据集...\")\n",
    "for zip_name in tqdm(os.listdir(original_dir)):\n",
    "    if zip_name.endswith('.zip'):\n",
    "        # 提取项目名称（去掉.zip后缀）\n",
    "        project_name = zip_name[:-4]\n",
    "        project_dir = os.path.join(dataset_dir, project_name)\n",
    "        \n",
    "        # 创建项目子目录\n",
    "        os.makedirs(project_dir, exist_ok=True)\n",
    "        \n",
    "        # 解压文件\n",
    "        zip_path = os.path.join(original_dir, zip_name)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(project_dir)\n",
    "        \n",
    "        # 查找并处理嵌套的项目目录\n",
    "        nested_dirs = [d for d in os.listdir(project_dir) \n",
    "                      if os.path.isdir(os.path.join(project_dir, d)) \n",
    "                      and not d.startswith('__MACOSX')]\n",
    "        \n",
    "        if nested_dirs:\n",
    "            # 假设第一个非__MACOSX目录是项目目录\n",
    "            nested_project_dir = os.path.join(project_dir, nested_dirs[0])\n",
    "            \n",
    "            # 移动所有文件到顶级项目目录\n",
    "            for item in os.listdir(nested_project_dir):\n",
    "                src = os.path.join(nested_project_dir, item)\n",
    "                dst = os.path.join(project_dir, item)\n",
    "                \n",
    "                # 如果是文件则移动，如果是目录则移动整个目录\n",
    "                if os.path.isfile(src):\n",
    "                    shutil.move(src, dst)\n",
    "                else:\n",
    "                    shutil.move(src, project_dir)\n",
    "            \n",
    "            # 删除嵌套的项目目录\n",
    "            shutil.rmtree(nested_project_dir)\n",
    "        \n",
    "        # 删除__MACOSX目录（如果存在）\n",
    "        macosx_dir = os.path.join(project_dir, '__MACOSX')\n",
    "        if os.path.exists(macosx_dir):\n",
    "            shutil.rmtree(macosx_dir)\n",
    "        \n",
    "        print(f\"已整理: {zip_name} -> {project_dir}\")\n",
    "\n",
    "print(\"所有数据集解压并整理完成！\")\n",
    "print(f\"整理后的数据集位于: {os.path.abspath(dataset_dir)}\")"
   ],
   "id": "9f4fe0b6413a2ab0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 预览前几行数据",
   "id": "36509c2f12691d31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 选择django项目作为示例\n",
    "project_name = \"django\"\n",
    "project_dir = os.path.join(\"dataset\", project_name)\n",
    "\n",
    "print(f\"预览项目: {project_name}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 列出所有文件\n",
    "files = os.listdir(project_dir)\n",
    "print(\"可用文件列表:\")\n",
    "for i, f in enumerate(files, 1):\n",
    "    print(f\"{i}. {f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 预览每个Excel文件\n",
    "excel_files = [f for f in files if f.endswith('.xlsx')]\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(project_dir, file)\n",
    "    print(f\"\\n文件: {file}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    try:\n",
    "        # 获取所有sheet名称\n",
    "        sheets = pd.ExcelFile(file_path).sheet_names\n",
    "        \n",
    "        if len(sheets) > 0:\n",
    "            sheet_name = sheets[0]\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            \n",
    "            print(f\"Sheet名称: {sheet_name}\")\n",
    "            print(f\"行数: {df.shape[0]}, 列数: {df.shape[1]}\")\n",
    "            \n",
    "            # 打印列名\n",
    "            print(\"\\n列名:\")\n",
    "            print(\", \".join(df.columns.tolist()))\n",
    "            \n",
    "            # 打印前1行数据\n",
    "            print(\"\\n前1行数据:\")\n",
    "            for i in range(min(1, len(df))):\n",
    "                row = df.iloc[i]\n",
    "                print(f\"行 {i+1}:\")\n",
    "                for col in df.columns:\n",
    "                    if col==\"comment_embedding\":\n",
    "                        print(f\"  {col}: {row[col]}\")\n",
    "                    else:\n",
    "                        print(f\"  {col}: {str(row[col])[0:10]}\")\n",
    "        else:\n",
    "            print(\"警告: 没有找到任何sheet\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件出错: {str(e)}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "\n",
    "# 预览txt文件\n",
    "# txt_files = [f for f in files if f.endswith('.txt')]\n",
    "# for file in txt_files:\n",
    "#     file_path = os.path.join(project_dir, file)\n",
    "#     print(f\"\\n文件: {file}\")\n",
    "#     print(\"-\"*50)\n",
    "#     \n",
    "#     try:\n",
    "#         with open(file_path, 'r') as f:\n",
    "#             # 尝试读取前5行\n",
    "#             lines = []\n",
    "#             for _ in range(5):\n",
    "#                 try:\n",
    "#                     lines.append(next(f).strip())\n",
    "#                 except StopIteration:\n",
    "#                     break\n",
    "#             \n",
    "#             print(\"前5行内容:\")\n",
    "#             for i, line in enumerate(lines, 1):\n",
    "#                 print(f\"{i}. {line}\")\n",
    "#                 \n",
    "#             # 如果是JSON格式，尝试解析\n",
    "#             if file == \"pr_time_dict.txt\":\n",
    "#                 print(\"\\n尝试解析为JSON:\")\n",
    "#                 f.seek(0)\n",
    "#                 try:\n",
    "#                     data = json.load(f)\n",
    "#                     print(f\"数据类型: {type(data)}\")\n",
    "#                     print(f\"键数量: {len(data)}\")\n",
    "#                     \n",
    "#                     # 显示前3个键值对\n",
    "#                     print(\"\\n前3个键值对:\")\n",
    "#                     for i, (k, v) in enumerate(data.items()):\n",
    "#                         if i >= 3:\n",
    "#                             break\n",
    "#                         print(f\"{k}: {v}\")\n",
    "#                 except json.JSONDecodeError:\n",
    "#                     print(\"无法解析为JSON格式\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"读取文件出错: {str(e)}\")\n",
    "#     \n",
    "#     print(\"=\"*50)"
   ],
   "id": "9c800c6b0c503aa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 以下数据处理分为三步完成\n",
    "\n",
    "## 第一步 数据合并\n",
    "\n",
    "## 第二步 数据清洗\n",
    "\n",
    "## 第三步 特征工程"
   ],
   "id": "81a4e76886574056"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 第一步 数据合并\n",
   "id": "17e40453e00d7045"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "修复pr_time_dict.txt的格式，方便作为json处理",
   "id": "e17fa0ac1d7c1f10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "\n",
    "dataset_dir = \"dataset\"\n",
    "projects = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "\n",
    "print(\"开始修复所有项目的 pr_time_dict.txt 文件...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for project in projects:\n",
    "    file_path = os.path.join(dataset_dir, project, \"pr_time_dict.txt\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"处理项目: {project}\")\n",
    "        \n",
    "        # 1. 读取文件内容\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "        \n",
    "        # 2. 特殊处理：替换 nan 为 None\n",
    "        content = content.replace('nan', 'None')\n",
    "        \n",
    "        # 3. 使用 ast.literal_eval 解析\n",
    "        time_dict = ast.literal_eval(content)\n",
    "        \n",
    "        # 4. 转换 None 为 null（JSON格式）\n",
    "        # 递归函数处理嵌套字典\n",
    "        def convert_none(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                return {k: convert_none(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_none(item) for item in obj]\n",
    "            elif obj is None:\n",
    "                return None  # JSON 中 None 会转为 null\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "        time_dict = convert_none(time_dict)\n",
    "        \n",
    "        # 5. 保存为标准JSON格式\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(time_dict, f, indent=2)\n",
    "        \n",
    "        print(f\"  成功修复并保存: {project}/pr_time_dict.txt\")\n",
    "        print(\"-\"*50)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  无法修复 {project}/pr_time_dict.txt: {str(e)}\")\n",
    "        # 备份原始文件\n",
    "        backup_path = file_path + \".bak\"\n",
    "        with open(backup_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        print(f\"  已创建备份文件: {backup_path}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "print(\"\\n所有项目修复完成!\")"
   ],
   "id": "a18cf372bf50fa9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T12:49:49.667598Z",
     "start_time": "2025-10-15T12:38:30.427011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 创建输出目录\n",
    "output_dir = \"processed_dataset/merged\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 获取所有项目列表\n",
    "dataset_dir = \"dataset\"\n",
    "projects = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "\n",
    "print(f\"开始合并项目数据，共 {len(projects)} 个项目\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 使用tqdm创建进度条\n",
    "for project in tqdm(projects, desc=\"处理项目\"):\n",
    "    project_dir = os.path.join(dataset_dir, project)\n",
    "    \n",
    "    print(f\"\\n处理项目: {project}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    try:\n",
    "        # 1. 读取基础表 - PR_info_add_conversation.xlsx\n",
    "        base_df = pd.read_excel(os.path.join(project_dir, \"PR_info_add_conversation.xlsx\"))\n",
    "        print(f\"  基础表读取成功，行数: {len(base_df)}\")\n",
    "        \n",
    "        # 2. 合并PR_features.xlsx\n",
    "        try:\n",
    "            pr_features = pd.read_excel(os.path.join(project_dir, \"PR_features.xlsx\"))\n",
    "            base_df = pd.merge(base_df, pr_features, on=\"number\", how=\"left\", suffixes=('', '_pr'))\n",
    "            print(f\"  合并PR特征，新增列数: {len(pr_features.columns)-1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  警告: 无法合并PR特征 - {str(e)}\")\n",
    "        \n",
    "        # 3. 合并author_features.xlsx\n",
    "        try:\n",
    "            author_features = pd.read_excel(os.path.join(project_dir, \"author_features.xlsx\"))\n",
    "            base_df = pd.merge(base_df, author_features, on=\"number\", how=\"left\", suffixes=('', '_author'))\n",
    "            print(f\"  合并作者特征，新增列数: {len(author_features.columns)-1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  警告: 无法合并作者特征 - {str(e)}\")\n",
    "        \n",
    "        # 4. 合并reviewer_features.xlsx\n",
    "        try:\n",
    "            reviewer_features = pd.read_excel(os.path.join(project_dir, \"reviewer_features.xlsx\"))\n",
    "            base_df = pd.merge(base_df, reviewer_features, on=\"number\", how=\"left\", suffixes=('', '_reviewer'))\n",
    "            print(f\"  合并评审者特征，新增列数: {len(reviewer_features.columns)-1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  警告: 无法合并评审者特征 - {str(e)}\")\n",
    "        \n",
    "        # 5. 合并project_features.xlsx\n",
    "        try:\n",
    "            project_features = pd.read_excel(os.path.join(project_dir, \"project_features.xlsx\"))\n",
    "            # 项目特征对所有PR相同，直接添加列\n",
    "            for col in project_features.columns:\n",
    "                if col != \"number\":\n",
    "                    base_df[col] = project_features[col].iloc[0]\n",
    "            print(f\"  合并项目特征，新增列数: {len(project_features.columns)-1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  警告: 无法合并项目特征 - {str(e)}\")\n",
    "        \n",
    "        # 6. 处理pr_time_dict.txt\n",
    "        try:\n",
    "            with open(os.path.join(project_dir, \"pr_time_dict.txt\"), 'r', encoding='utf-8') as f:\n",
    "                time_dict = json.load(f)\n",
    "            \n",
    "            # 创建时间特征DataFrame\n",
    "            time_df = pd.DataFrame.from_dict(time_dict, orient='index')\n",
    "            time_df.index = time_df.index.astype(int)  # 确保索引为整数\n",
    "            time_df.reset_index(inplace=True)\n",
    "            time_df.rename(columns={'index': 'number'}, inplace=True)\n",
    "            \n",
    "            # 合并到主表\n",
    "            base_df = pd.merge(base_df, time_df, on=\"number\", how=\"left\", suffixes=('', '_time'))\n",
    "            print(f\"  合并时间字典，新增列数: {len(time_df.columns)-1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  警告: 无法处理时间字典 - {str(e)}\")\n",
    "        \n",
    "        # 7. 处理评论信息（聚合） - 修复错误\n",
    "        try:\n",
    "            comment_df = pd.read_excel(os.path.join(project_dir, \"PR_comment_info.xlsx\"))\n",
    "            print(f\"  读取评论信息，行数: {len(comment_df)}\")\n",
    "            \n",
    "            # 修复错误：确保body列是字符串类型\n",
    "            if 'body' in comment_df.columns:\n",
    "                # 将非字符串转换为字符串\n",
    "                comment_df['body'] = comment_df['body'].apply(lambda x: str(x) if not pd.isna(x) else \"\")\n",
    "            \n",
    "            # 计算每个PR的聚合特征\n",
    "            comment_agg = comment_df.groupby('belong_to_PR').agg(\n",
    "                comment_count=pd.NamedAgg(column='id', aggfunc='count'),\n",
    "                last_comment_time=pd.NamedAgg(column='created_at', aggfunc='max')\n",
    "            ).reset_index()\n",
    "            \n",
    "            # 安全地计算平均评论长度\n",
    "            if 'body' in comment_df.columns:\n",
    "                # 计算每个评论的长度\n",
    "                comment_df['body_length'] = comment_df['body'].str.len()\n",
    "                # 计算每个PR的平均评论长度\n",
    "                avg_length = comment_df.groupby('belong_to_PR')['body_length'].mean().reset_index()\n",
    "                avg_length.rename(columns={'body_length': 'avg_comment_length'}, inplace=True)\n",
    "                # 合并到聚合表\n",
    "                comment_agg = pd.merge(comment_agg, avg_length, on='belong_to_PR', how='left')\n",
    "            \n",
    "            comment_agg.rename(columns={'belong_to_PR': 'number'}, inplace=True)\n",
    "            \n",
    "            # 合并到主表\n",
    "            base_df = pd.merge(base_df, comment_agg, on=\"number\", how=\"left\")\n",
    "            print(f\"  添加评论聚合特征，新增列数: {len(comment_agg.columns)-1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  警告: 无法处理评论信息 - {str(e)}\")\n",
    "        \n",
    "        # 8. 处理提交信息（聚合）\n",
    "        try:\n",
    "            commit_df = pd.read_excel(os.path.join(project_dir, \"PR_commit_info.xlsx\"))\n",
    "            print(f\"  读取提交信息，行数: {len(commit_df)}\")\n",
    "            \n",
    "            # 修复可能的错误：确保file_name_list是字符串\n",
    "            if 'file_name_list' in commit_df.columns:\n",
    "                commit_df['file_name_list'] = commit_df['file_name_list'].astype(str)\n",
    "            \n",
    "            # 计算每个PR的聚合特征\n",
    "            commit_agg = commit_df.groupby('belong_to_PR').agg(\n",
    "                commit_count=pd.NamedAgg(column='sha', aggfunc='count'),\n",
    "                total_additions=pd.NamedAgg(column='additions', aggfunc='sum'),\n",
    "                total_deletions=pd.NamedAgg(column='deletions', aggfunc='sum'),\n",
    "                total_files_changed=pd.NamedAgg(column='file_name_list', aggfunc=lambda x: x.nunique())\n",
    "            ).reset_index()\n",
    "            commit_agg.rename(columns={'belong_to_PR': 'number'}, inplace=True)\n",
    "            \n",
    "            # 合并到主表\n",
    "            base_df = pd.merge(base_df, commit_agg, on=\"number\", how=\"left\")\n",
    "            print(f\"  添加提交聚合特征，新增列数: {len(commit_agg.columns)-1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  警告: 无法处理提交信息 - {str(e)}\")\n",
    "        \n",
    "        # 保存合并后的数据集\n",
    "        output_path = os.path.join(output_dir, f\"{project}_merged.xlsx\")\n",
    "        base_df.to_excel(output_path, index=False)\n",
    "        print(f\"  合并完成! 总列数: {len(base_df.columns)}\")\n",
    "        print(f\"  已保存到: {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  处理失败: {str(e)}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "\n",
    "print(\"\\n所有项目处理完成!\")\n",
    "print(f\"合并后的数据集保存在: {os.path.abspath(output_dir)}\")"
   ],
   "id": "5c48d4a4b621b892",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始合并项目数据，共 10 个项目\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理项目:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理项目: django\n",
      "--------------------------------------------------\n",
      "  基础表读取成功，行数: 16976\n",
      "  合并PR特征，新增列数: 34\n",
      "  合并作者特征，新增列数: 15\n",
      "  合并评审者特征，新增列数: 16\n",
      "  合并项目特征，新增列数: 30\n",
      "  合并时间字典，新增列数: 4\n",
      "  读取评论信息，行数: 46204\n",
      "  添加评论聚合特征，新增列数: 3\n",
      "  读取提交信息，行数: 32888\n",
      "  添加提交聚合特征，新增列数: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理项目:  10%|█         | 1/10 [00:55<08:17, 55.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  合并完成! 总列数: 115\n",
      "  已保存到: processed_dataset/merged\\django_merged.xlsx\n",
      "==================================================\n",
      "\n",
      "处理项目: moby\n",
      "--------------------------------------------------\n",
      "  基础表读取成功，行数: 23515\n",
      "  合并PR特征，新增列数: 34\n",
      "  合并作者特征，新增列数: 15\n",
      "  合并评审者特征，新增列数: 16\n",
      "  合并项目特征，新增列数: 30\n",
      "  合并时间字典，新增列数: 4\n",
      "  读取评论信息，行数: 133170\n",
      "  添加评论聚合特征，新增列数: 3\n",
      "  读取提交信息，行数: 41736\n",
      "  添加提交聚合特征，新增列数: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理项目:  20%|██        | 2/10 [02:16<09:22, 70.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  合并完成! 总列数: 115\n",
      "  已保存到: processed_dataset/merged\\moby_merged.xlsx\n",
      "==================================================\n",
      "\n",
      "处理项目: opencv\n",
      "--------------------------------------------------\n",
      "  基础表读取成功，行数: 13972\n",
      "  合并PR特征，新增列数: 34\n",
      "  合并作者特征，新增列数: 15\n",
      "  合并评审者特征，新增列数: 16\n",
      "  合并项目特征，新增列数: 30\n",
      "  合并时间字典，新增列数: 4\n",
      "  读取评论信息，行数: 33026\n",
      "  添加评论聚合特征，新增列数: 3\n",
      "  读取提交信息，行数: 44248\n",
      "  添加提交聚合特征，新增列数: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理项目:  30%|███       | 3/10 [03:00<06:50, 58.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  合并完成! 总列数: 115\n",
      "  已保存到: processed_dataset/merged\\opencv_merged.xlsx\n",
      "==================================================\n",
      "\n",
      "处理项目: react\n",
      "--------------------------------------------------\n",
      "  基础表读取成功，行数: 13671\n",
      "  合并PR特征，新增列数: 34\n",
      "  合并作者特征，新增列数: 15\n",
      "  合并评审者特征，新增列数: 16\n",
      "  合并项目特征，新增列数: 30\n",
      "  合并时间字典，新增列数: 4\n",
      "  读取评论信息，行数: 47565\n",
      "  添加评论聚合特征，新增列数: 3\n",
      "  读取提交信息，行数: 33376\n",
      "  添加提交聚合特征，新增列数: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理项目:  40%|████      | 4/10 [03:46<05:20, 53.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  合并完成! 总列数: 115\n",
      "  已保存到: processed_dataset/merged\\react_merged.xlsx\n",
      "==================================================\n",
      "\n",
      "处理项目: salt\n",
      "--------------------------------------------------\n",
      "  基础表读取成功，行数: 39025\n",
      "  合并PR特征，新增列数: 34\n",
      "  合并作者特征，新增列数: 15\n",
      "  合并评审者特征，新增列数: 16\n",
      "  合并项目特征，新增列数: 30\n",
      "  合并时间字典，新增列数: 4\n",
      "  读取评论信息，行数: 77180\n",
      "  添加评论聚合特征，新增列数: 3\n",
      "  读取提交信息，行数: 138402\n",
      "  添加提交聚合特征，新增列数: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理项目:  50%|█████     | 5/10 [05:54<06:41, 80.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  合并完成! 总列数: 115\n",
      "  已保存到: processed_dataset/merged\\salt_merged.xlsx\n",
      "==================================================\n",
      "\n",
      "处理项目: scikit-learn\n",
      "--------------------------------------------------\n",
      "  基础表读取成功，行数: 15687\n",
      "  合并PR特征，新增列数: 34\n",
      "  合并作者特征，新增列数: 15\n",
      "  合并评审者特征，新增列数: 16\n",
      "  合并项目特征，新增列数: 30\n",
      "  合并时间字典，新增列数: 4\n",
      "  读取评论信息，行数: 89562\n",
      "  添加评论聚合特征，新增列数: 3\n",
      "  读取提交信息，行数: 93513\n",
      "  添加提交聚合特征，新增列数: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理项目:  60%|██████    | 6/10 [06:57<04:57, 74.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  合并完成! 总列数: 115\n",
      "  已保存到: processed_dataset/merged\\scikit-learn_merged.xlsx\n",
      "==================================================\n",
      "\n",
      "处理项目: symfony\n",
      "--------------------------------------------------\n",
      "  基础表读取成功，行数: 30392\n",
      "  合并PR特征，新增列数: 34\n",
      "  合并作者特征，新增列数: 15\n",
      "  合并评审者特征，新增列数: 16\n",
      "  合并项目特征，新增列数: 30\n",
      "  合并时间字典，新增列数: 4\n",
      "  读取评论信息，行数: 111274\n",
      "  添加评论聚合特征，新增列数: 3\n",
      "  读取提交信息，行数: 54964\n",
      "  添加提交聚合特征，新增列数: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理项目:  70%|███████   | 7/10 [08:43<04:15, 85.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  合并完成! 总列数: 115\n",
      "  已保存到: processed_dataset/merged\\symfony_merged.xlsx\n",
      "==================================================\n",
      "\n",
      "处理项目: tensorflow\n",
      "--------------------------------------------------\n",
      "  基础表读取成功，行数: 23130\n",
      "  合并PR特征，新增列数: 34\n",
      "  合并作者特征，新增列数: 15\n",
      "  合并评审者特征，新增列数: 16\n",
      "  合并项目特征，新增列数: 30\n",
      "  合并时间字典，新增列数: 4\n",
      "  读取评论信息，行数: 77449\n",
      "  添加评论聚合特征，新增列数: 3\n",
      "  读取提交信息，行数: 83113\n",
      "  添加提交聚合特征，新增列数: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理项目:  80%|████████  | 8/10 [10:04<02:47, 83.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  合并完成! 总列数: 115\n",
      "  已保存到: processed_dataset/merged\\tensorflow_merged.xlsx\n",
      "==================================================\n",
      "\n",
      "处理项目: terraform\n",
      "--------------------------------------------------\n",
      "  基础表读取成功，行数: 13219\n",
      "  合并PR特征，新增列数: 34\n",
      "  合并作者特征，新增列数: 15\n",
      "  合并评审者特征，新增列数: 16\n",
      "  合并项目特征，新增列数: 30\n",
      "  合并时间字典，新增列数: 4\n",
      "  读取评论信息，行数: 42128\n",
      "  添加评论聚合特征，新增列数: 3\n",
      "  读取提交信息，行数: 31384\n",
      "  添加提交聚合特征，新增列数: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理项目:  90%|█████████ | 9/10 [10:50<01:11, 71.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  合并完成! 总列数: 115\n",
      "  已保存到: processed_dataset/merged\\terraform_merged.xlsx\n",
      "==================================================\n",
      "\n",
      "处理项目: yii2\n",
      "--------------------------------------------------\n",
      "  基础表读取成功，行数: 8040\n",
      "  合并PR特征，新增列数: 34\n",
      "  合并作者特征，新增列数: 15\n",
      "  合并评审者特征，新增列数: 16\n",
      "  合并项目特征，新增列数: 30\n",
      "  合并时间字典，新增列数: 4\n",
      "  读取评论信息，行数: 25643\n",
      "  添加评论聚合特征，新增列数: 3\n",
      "  读取提交信息，行数: 23581\n",
      "  添加提交聚合特征，新增列数: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理项目: 100%|██████████| 10/10 [11:18<00:00, 67.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  合并完成! 总列数: 115\n",
      "  已保存到: processed_dataset/merged\\yii2_merged.xlsx\n",
      "==================================================\n",
      "\n",
      "所有项目处理完成!\n",
      "合并后的数据集保存在: D:\\hehaochuan\\codes\\machine_learning\\lab2\\processed_dataset\\merged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 第二步 数据清洗",
   "id": "ab6efc8a051b82f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "预览合并后数据结构",
   "id": "e97011c01a8ad5aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T13:17:20.298858Z",
     "start_time": "2025-10-15T13:17:07.028974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 选择要预览的项目\n",
    "project_name = \"django\"  # 替换为您想预览的项目名称\n",
    "\n",
    "# 合并数据文件路径\n",
    "merged_file = os.path.join(\"processed_dataset\", \"merged\", f\"{project_name}_merged.xlsx\")\n",
    "\n",
    "print(f\"预览项目: {project_name}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # 读取Excel文件\n",
    "    df = pd.read_excel(merged_file)\n",
    "    \n",
    "    # 显示基本信息\n",
    "    print(f\"文件路径: {merged_file}\")\n",
    "    print(f\"总行数: {len(df)}\")\n",
    "    print(f\"总列数: {len(df.columns)}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 显示所有列名\n",
    "    print(\"\\n所有列名:\")\n",
    "    print(\"-\"*50)\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"{i}. {col}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 显示第一行数据\n",
    "    print(\"\\n第一行数据:\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # 获取第一行数据\n",
    "    first_row = df.iloc[0]\n",
    "    \n",
    "    # 按列分组显示\n",
    "    groups = {\n",
    "        \"基本信息\": [\"number\", \"state\", \"title\", \"author\", \"merged\", \"conversation\", \"closed_at\"],\n",
    "        \"时间信息\": [\"created_at\", \"updated_at\", \"merged_at\", \"closed_at\", \"last_comment_time\"],\n",
    "        \"变更统计\": [\"additions\", \"deletions\", \"changed_files\", \"commit_count\", \"comment_count\"],\n",
    "        \"技术特征\": [\"has_test\", \"has_feature\", \"has_bug\", \"has_document\", \"file_type\"],\n",
    "        \"作者特征\": [\"experience\", \"participation\", \"change_num\", \"merge_proportion\"],\n",
    "        \"评审者特征\": [\"avg_comments\", \"avg_round\", \"avg_duration\"],\n",
    "        \"项目特征\": [\"project_age\", \"team_size\", \"changes_per_week\"],\n",
    "        \"聚合特征\": [\"total_additions\", \"total_deletions\", \"total_files_changed\", \"avg_comment_length\"]\n",
    "    }\n",
    "    \n",
    "    # 显示每组的关键列\n",
    "    for group_name, columns in groups.items():\n",
    "        print(f\"\\n{group_name}组:\")\n",
    "        print(\"-\"*20)\n",
    "        \n",
    "        # 只显示该组中实际存在的列\n",
    "        existing_columns = [col for col in columns if col in df.columns]\n",
    "        \n",
    "        for col in existing_columns:\n",
    "            value = first_row[col]\n",
    "            \n",
    "            # 处理长文本和嵌入向量\n",
    "            if isinstance(value, str) and len(value) > 100:\n",
    "                value = value[:100] + \"...\"\n",
    "            elif isinstance(value, list):\n",
    "                value = f\"列表[{len(value)}项]\"\n",
    "            \n",
    "            print(f\"  {col}: {value}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 保存列名到文件以便详细查看\n",
    "    columns_file = os.path.join(\"processed_dataset\", f\"{project_name}_columns.txt\")\n",
    "    with open(columns_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"{project_name} 数据集列名列表 (共{len(df.columns)}列):\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            f.write(f\"{i}. {col}\\n\")\n",
    "    \n",
    "    print(f\"\\n完整列名列表已保存到: {columns_file}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"预览失败: {str(e)}\")\n",
    "    print(\"=\"*50)"
   ],
   "id": "34d8492ca1cd13d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预览项目: django\n",
      "==================================================\n",
      "文件路径: processed_dataset\\merged\\django_merged.xlsx\n",
      "总行数: 16976\n",
      "总列数: 115\n",
      "==================================================\n",
      "\n",
      "所有列名:\n",
      "--------------------------------------------------\n",
      "1. number\n",
      "2. state\n",
      "3. title\n",
      "4. author\n",
      "5. body\n",
      "6. created_at\n",
      "7. updated_at\n",
      "8. merged_at\n",
      "9. merged\n",
      "10. comments\n",
      "11. review_comments\n",
      "12. commits\n",
      "13. additions\n",
      "14. deletions\n",
      "15. changed_files\n",
      "16. conversation\n",
      "17. closed_at\n",
      "18. directory_num\n",
      "19. language_num\n",
      "20. file_type\n",
      "21. has_test\n",
      "22. has_feature\n",
      "23. has_bug\n",
      "24. has_document\n",
      "25. has_improve\n",
      "26. has_refactor\n",
      "27. title_length\n",
      "28. title_readability\n",
      "29. title_embedding\n",
      "30. body_length\n",
      "31. body_readability\n",
      "32. body_embedding\n",
      "33. lines_added\n",
      "34. lines_deleted\n",
      "35. segs_added\n",
      "36. segs_deleted\n",
      "37. segs_updated\n",
      "38. files_added\n",
      "39. files_deleted\n",
      "40. files_updated\n",
      "41. modify_proportion\n",
      "42. modify_entropy\n",
      "43. test_churn\n",
      "44. non_test_churn\n",
      "45. reviewer_num\n",
      "46. bot_reviewer_num\n",
      "47. is_reviewed\n",
      "48. comment_num\n",
      "49. comment_length\n",
      "50. comment_embedding\n",
      "51. last_comment_mention\n",
      "52. name\n",
      "53. experience\n",
      "54. is_reviewer\n",
      "55. change_num\n",
      "56. participation\n",
      "57. changes_per_week\n",
      "58. avg_round\n",
      "59. avg_duration\n",
      "60. merge_proportion\n",
      "61. degree_centrality\n",
      "62. closeness_centrality\n",
      "63. betweenness_centrality\n",
      "64. eigenvector_centrality\n",
      "65. clustering_coefficient\n",
      "66. k_coreness\n",
      "67. name_reviewer\n",
      "68. experience_reviewer\n",
      "69. is_author\n",
      "70. change_num_reviewer\n",
      "71. participation_reviewer\n",
      "72. avg_comments\n",
      "73. avg_files\n",
      "74. avg_round_reviewer\n",
      "75. avg_duration_reviewer\n",
      "76. merge_proportion_reviewer\n",
      "77. degree_centrality_reviewer\n",
      "78. closeness_centrality_reviewer\n",
      "79. betweenness_centrality_reviewer\n",
      "80. eigenvector_centrality_reviewer\n",
      "81. clustering_coefficient_reviewer\n",
      "82. k_coreness_reviewer\n",
      "83. project_age\n",
      "84. open_changes\n",
      "85. author_num\n",
      "86. team_size\n",
      "87. changes_per_author\n",
      "88. changes_per_reviewer\n",
      "89. avg_lines\n",
      "90. avg_segs\n",
      "91. add_per_week\n",
      "92. del_per_week\n",
      "93. avg_reviewers\n",
      "94. avg_rounds\n",
      "95. avg_rounds_merged\n",
      "96. avg_duration_merged\n",
      "97. avg_churn_merged\n",
      "98. avg_files_merged\n",
      "99. avg_comments_merged\n",
      "100. avg_rounds_abandoned\n",
      "101. avg_duration_abandoned\n",
      "102. avg_churn_abandoned\n",
      "103. avg_files_abandoned\n",
      "104. avg_comments_abandoned\n",
      "105. created_at_time\n",
      "106. updated_at_time\n",
      "107. merged_at_time\n",
      "108. closed_at_time\n",
      "109. comment_count\n",
      "110. last_comment_time\n",
      "111. avg_comment_length\n",
      "112. commit_count\n",
      "113. total_additions\n",
      "114. total_deletions\n",
      "115. total_files_changed\n",
      "==================================================\n",
      "\n",
      "第一行数据:\n",
      "--------------------------------------------------\n",
      "\n",
      "基本信息组:\n",
      "--------------------\n",
      "  number: 1\n",
      "  state: closed\n",
      "  title: Change phone2numeric to use generator expressions instead of lambdas\n",
      "  author: ghost\n",
      "  merged: False\n",
      "  conversation: 0\n",
      "  closed_at: 2012-04-28T07:34:01Z\n",
      "\n",
      "时间信息组:\n",
      "--------------------\n",
      "  created_at: 2012-04-28T07:32:22Z\n",
      "  updated_at: 2014-06-27T23:53:10Z\n",
      "  merged_at: nan\n",
      "  closed_at: 2012-04-28T07:34:01Z\n",
      "  last_comment_time: nan\n",
      "\n",
      "变更统计组:\n",
      "--------------------\n",
      "  additions: 6\n",
      "  deletions: 7\n",
      "  changed_files: 1\n",
      "  commit_count: 2.0\n",
      "  comment_count: nan\n",
      "\n",
      "技术特征组:\n",
      "--------------------\n",
      "  has_test: False\n",
      "  has_feature: False\n",
      "  has_bug: False\n",
      "  has_document: False\n",
      "  file_type: 1\n",
      "\n",
      "作者特征组:\n",
      "--------------------\n",
      "  experience: 4102\n",
      "  participation: 5.89066918001885e-05\n",
      "  change_num: 1\n",
      "  merge_proportion: 0\n",
      "\n",
      "评审者特征组:\n",
      "--------------------\n",
      "  avg_comments: 46204\n",
      "  avg_round: 2.0\n",
      "  avg_duration: 790\n",
      "\n",
      "项目特征组:\n",
      "--------------------\n",
      "  project_age: 1\n",
      "  team_size: 2\n",
      "  changes_per_week: 7\n",
      "\n",
      "聚合特征组:\n",
      "--------------------\n",
      "  total_additions: 7.0\n",
      "  total_deletions: 8.0\n",
      "  total_files_changed: 1.0\n",
      "  avg_comment_length: nan\n",
      "==================================================\n",
      "\n",
      "完整列名列表已保存到: processed_dataset\\django_columns.txt\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "统计缺失值占比和重复率（pr）",
   "id": "72b1885f9e5ffea0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T13:27:44.369271Z",
     "start_time": "2025-10-15T13:25:10.155848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 创建统计结果目录\n",
    "stats_dir = \"processed_dataset/stats\"\n",
    "os.makedirs(stats_dir, exist_ok=True)\n",
    "\n",
    "# 获取所有项目列表\n",
    "dataset_dir = \"dataset\"\n",
    "projects = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "\n",
    "print(f\"开始对所有项目进行统计分析，共 {len(projects)} 个项目\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 创建汇总数据框\n",
    "summary_missing = pd.DataFrame()\n",
    "summary_duplicates = pd.DataFrame(columns=[\"Project\", \"Duplicate_Count\", \"Duplicate_Percentage\"])\n",
    "\n",
    "# 遍历所有项目\n",
    "for project in tqdm(projects, desc=\"分析项目\"):\n",
    "    project_dir = os.path.join(dataset_dir, project)\n",
    "    merged_file = os.path.join(\"processed_dataset\", \"merged\", f\"{project}_merged.xlsx\")\n",
    "    \n",
    "    print(f\"\\n分析项目: {project}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    try:\n",
    "        # 读取Excel文件\n",
    "        df = pd.read_excel(merged_file)\n",
    "        \n",
    "        # 显示基本信息\n",
    "        print(f\"  文件路径: {merged_file}\")\n",
    "        print(f\"  总行数: {len(df)}\")\n",
    "        print(f\"  总列数: {len(df.columns)}\")\n",
    "        \n",
    "        # 1. 缺失值统计\n",
    "        missing_data = df.isnull().sum().reset_index()\n",
    "        missing_data.columns = ['Column', 'Missing_Count']\n",
    "        missing_data['Missing_Percentage'] = (missing_data['Missing_Count'] / len(df)) * 100\n",
    "        \n",
    "        # 添加到汇总表\n",
    "        missing_data['Project'] = project\n",
    "        summary_missing = pd.concat([summary_missing, missing_data], ignore_index=True)\n",
    "        \n",
    "        # 打印高缺失率列\n",
    "        high_missing = missing_data[missing_data['Missing_Percentage'] > 10]\n",
    "        if len(high_missing) > 0:\n",
    "            print(f\"  高缺失率列 (>10%):\")\n",
    "            for _, row in high_missing.iterrows():\n",
    "                print(f\"    {row['Column']}: {row['Missing_Percentage']:.2f}%\")\n",
    "        else:\n",
    "            print(\"  没有高缺失率列 (>10%)\")\n",
    "        \n",
    "        # 2. 重复值统计\n",
    "        duplicate_counts = df['number'].duplicated().sum()\n",
    "        duplicate_percentage = (duplicate_counts / len(df)) * 100\n",
    "        \n",
    "        # 添加到汇总表 - 修复警告问题\n",
    "        new_row = pd.DataFrame({\n",
    "            \"Project\": [project],\n",
    "            \"Duplicate_Count\": [duplicate_counts],\n",
    "            \"Duplicate_Percentage\": [duplicate_percentage]\n",
    "        })\n",
    "        \n",
    "        summary_duplicates = pd.concat([summary_duplicates, new_row], ignore_index=True)\n",
    "        \n",
    "        print(f\"  PR number重复数量: {duplicate_counts} ({duplicate_percentage:.2f}%)\")\n",
    "        \n",
    "        # 3. 保存项目统计结果\n",
    "        project_stats_dir = os.path.join(stats_dir, project)\n",
    "        os.makedirs(project_stats_dir, exist_ok=True)\n",
    "        \n",
    "        # 保存缺失值统计\n",
    "        missing_file = os.path.join(project_stats_dir, f\"{project}_missing_stats.csv\")\n",
    "        missing_data.to_csv(missing_file, index=False)\n",
    "        print(f\"  缺失值统计已保存到: {missing_file}\")\n",
    "        \n",
    "        # 保存重复值详情\n",
    "        if duplicate_counts > 0:\n",
    "            duplicates = df[df['number'].duplicated(keep=False)]\n",
    "            duplicates_file = os.path.join(project_stats_dir, f\"{project}_duplicates.csv\")\n",
    "            duplicates.to_csv(duplicates_file, index=False)\n",
    "            print(f\"  重复值详情已保存到: {duplicates_file}\")\n",
    "        \n",
    "        print(\"-\"*50)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  分析失败: {str(e)}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "# 保存汇总统计结果\n",
    "print(\"\\n保存汇总统计结果\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 缺失值汇总\n",
    "summary_missing_file = os.path.join(stats_dir, \"all_projects_missing_summary.csv\")\n",
    "summary_missing.to_csv(summary_missing_file, index=False)\n",
    "print(f\"所有项目缺失值汇总已保存到: {summary_missing_file}\")\n",
    "\n",
    "# 重复值汇总\n",
    "summary_duplicates_file = os.path.join(stats_dir, \"all_projects_duplicates_summary.csv\")\n",
    "summary_duplicates.to_csv(summary_duplicates_file, index=False)\n",
    "print(f\"所有项目重复值汇总已保存到: {summary_duplicates_file}\")\n",
    "\n",
    "# 高缺失率列分析\n",
    "if not summary_missing.empty:\n",
    "    high_missing_summary = summary_missing[summary_missing['Missing_Percentage'] > 10]\n",
    "    if not high_missing_summary.empty:\n",
    "        high_missing_file = os.path.join(stats_dir, \"high_missing_columns.csv\")\n",
    "        high_missing_summary.to_csv(high_missing_file, index=False)\n",
    "        print(f\"高缺失率列详情已保存到: {high_missing_file}\")\n",
    "        \n",
    "        # 按列分组分析\n",
    "        column_missing_stats = high_missing_summary.groupby('Column')['Missing_Percentage'].agg(['mean', 'max', 'min'])\n",
    "        column_missing_stats.columns = ['Average_Missing', 'Max_Missing', 'Min_Missing']\n",
    "        column_missing_stats = column_missing_stats.sort_values(by='Average_Missing', ascending=False)\n",
    "        \n",
    "        column_stats_file = os.path.join(stats_dir, \"column_missing_stats.csv\")\n",
    "        column_missing_stats.to_csv(column_stats_file)\n",
    "        print(f\"列缺失率统计已保存到: {column_stats_file}\")\n",
    "        \n",
    "        print(\"\\n高缺失率列统计:\")\n",
    "        print(column_missing_stats.head(10))\n",
    "    else:\n",
    "        print(\"没有高缺失率列 (>10%)\")\n",
    "else:\n",
    "    print(\"缺失值汇总为空，无法进行高缺失率列分析\")\n",
    "\n",
    "print(\"\\n所有项目统计分析完成!\")\n",
    "print(\"=\"*50)"
   ],
   "id": "6d1e9f5a32322a49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对所有项目进行统计分析，共 10 个项目\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分析项目:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "分析项目: django\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hehaochuan\\AppData\\Local\\Temp\\ipykernel_146012\\925333143.py:67: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  summary_duplicates = pd.concat([summary_duplicates, new_row], ignore_index=True)\n",
      "分析项目:  10%|█         | 1/10 [00:13<01:58, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  文件路径: processed_dataset\\merged\\django_merged.xlsx\n",
      "  总行数: 16976\n",
      "  总列数: 115\n",
      "  高缺失率列 (>10%):\n",
      "    body: 21.65%\n",
      "    merged_at: 51.44%\n",
      "    name_reviewer: 23.29%\n",
      "    merged_at_time: 51.44%\n",
      "    comment_count: 23.29%\n",
      "    last_comment_time: 23.29%\n",
      "    avg_comment_length: 23.29%\n",
      "  PR number重复数量: 0 (0.00%)\n",
      "  缺失值统计已保存到: processed_dataset/stats\\django\\django_missing_stats.csv\n",
      "--------------------------------------------------\n",
      "\n",
      "分析项目: moby\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分析项目:  20%|██        | 2/10 [00:32<02:15, 16.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  文件路径: processed_dataset\\merged\\moby_merged.xlsx\n",
      "  总行数: 23515\n",
      "  总列数: 115\n",
      "  高缺失率列 (>10%):\n",
      "    merged_at: 21.26%\n",
      "    merged_at_time: 21.26%\n",
      "  PR number重复数量: 0 (0.00%)\n",
      "  缺失值统计已保存到: processed_dataset/stats\\moby\\moby_missing_stats.csv\n",
      "--------------------------------------------------\n",
      "\n",
      "分析项目: opencv\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分析项目:  30%|███       | 3/10 [00:48<01:53, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  文件路径: processed_dataset\\merged\\opencv_merged.xlsx\n",
      "  总行数: 13972\n",
      "  总列数: 115\n",
      "  高缺失率列 (>10%):\n",
      "    body: 11.72%\n",
      "    merged_at: 19.33%\n",
      "    name_reviewer: 19.38%\n",
      "    merged_at_time: 19.33%\n",
      "    comment_count: 19.38%\n",
      "    last_comment_time: 19.38%\n",
      "    avg_comment_length: 19.38%\n",
      "  PR number重复数量: 0 (0.00%)\n",
      "  缺失值统计已保存到: processed_dataset/stats\\opencv\\opencv_missing_stats.csv\n",
      "--------------------------------------------------\n",
      "\n",
      "分析项目: react\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分析项目:  40%|████      | 4/10 [00:58<01:23, 13.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  文件路径: processed_dataset\\merged\\react_merged.xlsx\n",
      "  总行数: 13671\n",
      "  总列数: 115\n",
      "  高缺失率列 (>10%):\n",
      "    merged_at: 33.17%\n",
      "    name_reviewer: 12.44%\n",
      "    merged_at_time: 33.17%\n",
      "    comment_count: 12.44%\n",
      "    last_comment_time: 12.44%\n",
      "    avg_comment_length: 12.44%\n",
      "  PR number重复数量: 0 (0.00%)\n",
      "  缺失值统计已保存到: processed_dataset/stats\\react\\react_missing_stats.csv\n",
      "--------------------------------------------------\n",
      "\n",
      "分析项目: salt\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分析项目:  50%|█████     | 5/10 [01:27<01:35, 19.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  文件路径: processed_dataset\\merged\\salt_merged.xlsx\n",
      "  总行数: 39025\n",
      "  总列数: 115\n",
      "  高缺失率列 (>10%):\n",
      "    body: 16.46%\n",
      "    merged_at: 12.39%\n",
      "    name_reviewer: 39.27%\n",
      "    merged_at_time: 12.39%\n",
      "    comment_count: 39.27%\n",
      "    last_comment_time: 39.27%\n",
      "    avg_comment_length: 39.27%\n",
      "  PR number重复数量: 0 (0.00%)\n",
      "  缺失值统计已保存到: processed_dataset/stats\\salt\\salt_missing_stats.csv\n",
      "--------------------------------------------------\n",
      "\n",
      "分析项目: scikit-learn\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分析项目:  60%|██████    | 6/10 [01:38<01:06, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  文件路径: processed_dataset\\merged\\scikit-learn_merged.xlsx\n",
      "  总行数: 15687\n",
      "  总列数: 115\n",
      "  高缺失率列 (>10%):\n",
      "    merged_at: 31.23%\n",
      "    modify_entropy: 17.73%\n",
      "    name_reviewer: 17.37%\n",
      "    merged_at_time: 31.23%\n",
      "    comment_count: 17.37%\n",
      "    last_comment_time: 17.37%\n",
      "    avg_comment_length: 17.37%\n",
      "  PR number重复数量: 0 (0.00%)\n",
      "  缺失值统计已保存到: processed_dataset/stats\\scikit-learn\\scikit-learn_missing_stats.csv\n",
      "--------------------------------------------------\n",
      "\n",
      "分析项目: symfony\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分析项目:  70%|███████   | 7/10 [02:00<00:55, 18.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  文件路径: processed_dataset\\merged\\symfony_merged.xlsx\n",
      "  总行数: 30392\n",
      "  总列数: 115\n",
      "  高缺失率列 (>10%):\n",
      "    merged_at: 33.23%\n",
      "    name_reviewer: 17.08%\n",
      "    merged_at_time: 33.23%\n",
      "    comment_count: 17.08%\n",
      "    last_comment_time: 17.08%\n",
      "    avg_comment_length: 17.08%\n",
      "  PR number重复数量: 0 (0.00%)\n",
      "  缺失值统计已保存到: processed_dataset/stats\\symfony\\symfony_missing_stats.csv\n",
      "--------------------------------------------------\n",
      "\n",
      "分析项目: tensorflow\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分析项目:  80%|████████  | 8/10 [02:17<00:35, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  文件路径: processed_dataset\\merged\\tensorflow_merged.xlsx\n",
      "  总行数: 23130\n",
      "  总列数: 115\n",
      "  高缺失率列 (>10%):\n",
      "    body: 17.66%\n",
      "    merged_at: 30.63%\n",
      "    name_reviewer: 33.23%\n",
      "    merged_at_time: 30.63%\n",
      "    comment_count: 33.23%\n",
      "    last_comment_time: 33.23%\n",
      "    avg_comment_length: 33.23%\n",
      "  PR number重复数量: 0 (0.00%)\n",
      "  缺失值统计已保存到: processed_dataset/stats\\tensorflow\\tensorflow_missing_stats.csv\n",
      "--------------------------------------------------\n",
      "\n",
      "分析项目: terraform\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分析项目:  90%|█████████ | 9/10 [02:28<00:15, 15.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  文件路径: processed_dataset\\merged\\terraform_merged.xlsx\n",
      "  总行数: 13219\n",
      "  总列数: 115\n",
      "  高缺失率列 (>10%):\n",
      "    merged_at: 18.47%\n",
      "    merged_at_time: 18.47%\n",
      "  PR number重复数量: 0 (0.00%)\n",
      "  缺失值统计已保存到: processed_dataset/stats\\terraform\\terraform_missing_stats.csv\n",
      "--------------------------------------------------\n",
      "\n",
      "分析项目: yii2\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "分析项目: 100%|██████████| 10/10 [02:34<00:00, 15.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  文件路径: processed_dataset\\merged\\yii2_merged.xlsx\n",
      "  总行数: 8040\n",
      "  总列数: 115\n",
      "  高缺失率列 (>10%):\n",
      "    body: 20.85%\n",
      "    merged_at: 36.58%\n",
      "    merged_at_time: 36.58%\n",
      "  PR number重复数量: 0 (0.00%)\n",
      "  缺失值统计已保存到: processed_dataset/stats\\yii2\\yii2_missing_stats.csv\n",
      "--------------------------------------------------\n",
      "\n",
      "保存汇总统计结果\n",
      "==================================================\n",
      "所有项目缺失值汇总已保存到: processed_dataset/stats\\all_projects_missing_summary.csv\n",
      "所有项目重复值汇总已保存到: processed_dataset/stats\\all_projects_duplicates_summary.csv\n",
      "高缺失率列详情已保存到: processed_dataset/stats\\high_missing_columns.csv\n",
      "列缺失率统计已保存到: processed_dataset/stats\\column_missing_stats.csv\n",
      "\n",
      "高缺失率列统计:\n",
      "                    Average_Missing  Max_Missing  Min_Missing\n",
      "Column                                                       \n",
      "merged_at_time            28.772269    51.437323    12.386931\n",
      "merged_at                 28.772269    51.437323    12.386931\n",
      "avg_comment_length        23.150916    39.274824    12.435082\n",
      "comment_count             23.150916    39.274824    12.435082\n",
      "name_reviewer             23.150916    39.274824    12.435082\n",
      "last_comment_time         23.150916    39.274824    12.435082\n",
      "modify_entropy            17.728055    17.728055    17.728055\n",
      "body                      17.668609    21.654100    11.723447\n",
      "\n",
      "所有项目统计分析完成!\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 完成数据清洗\n",
    "\n",
    "### 特殊说明\n",
    "对于state是open，则merge_at,closed_at,update_at等允许缺失而不进行填充\n",
    "\n",
    "### 删除重复记录：\n",
    "按 number列删除重复的PR记录\n",
    "保留最新的记录（基于 updated_at）\n",
    "### 删除 merged_at_time 列：\n",
    "检测并删除 merged_at_time,closed_at_time...列（与 merged_at...重复）\n",
    "### 统一时间格式：\n",
    "将时间列转换为 datetime 格式\n",
    "使用 fix_time_format函数处理各种时间格式\n",
    "确保时间顺序正确（created_at≤ updated_at≤ closed_at）\n",
    "### 处理 closed_at 缺失值：\n",
    "用 updated_at填充 closed_at\n",
    "如果 updated_at也缺失，则删除该行记录\n",
    "### 处理其他缺失值：\n",
    "高缺失率列采用特定策略填充\n",
    "数值列用中位数填充\n",
    "时间列用前后值填充\n",
    "分类列用众数填充\n",
    "文本列用空字符串填充\n",
    "### 处理异常值：\n",
    "时间异常：确保时间顺序正确\n",
    "数值异常：负值设为0，极端值进行缩尾处理\n",
    "### 数据类型转换：\n",
    "布尔列转换为0/1整数\n",
    "时间列统一为datetime类型\n",
    "### 文本清洗\n",
    "移除HTML标签和特殊字符\n",
    "截断过长的文本\n",
    "### 生成清洗报告：\n",
    "记录清洗前后的记录数变化\n",
    "记录每列的缺失值处理情况\n",
    "保存为文本文件"
   ],
   "id": "4ac7e01cd54865f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T01:11:26.452303Z",
     "start_time": "2025-10-16T00:59:07.146799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json\n",
    "from pandas.api.types import is_datetime64_any_dtype, is_datetime64tz_dtype, DatetimeTZDtype\n",
    "\n",
    "def clean_project_data(df):\n",
    "    \"\"\"\n",
    "    清洗单个项目的数据，并返回清洗后的DataFrame和统计信息\n",
    "    \"\"\"\n",
    "    # 初始化统计信息字典\n",
    "    stats = {\n",
    "        \"original_rows\": len(df),\n",
    "        \"rows_removed\": 0,\n",
    "        \"columns_processed\": {},\n",
    "        \"filled_values\": 0,\n",
    "        \"removed_values\": 0\n",
    "    }\n",
    "    \n",
    "    # 1. 删除重复记录\n",
    "    original_rows = len(df)\n",
    "    df = df.sort_values('updated_at', ascending=False)\n",
    "    df = df.drop_duplicates('number', keep='first')\n",
    "    stats[\"rows_removed\"] += original_rows - len(df)\n",
    "    \n",
    "    # 2. 删除重复的时间列\n",
    "    # 这些列与 created_at, updated_at, closed_at 重复\n",
    "    duplicate_time_cols = [\n",
    "        'created_at_time', 'updated_at_time', 'closed_at_time', 'merged_at_time'\n",
    "    ]\n",
    "    \n",
    "    for col in duplicate_time_cols:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "            stats[\"columns_processed\"][col] = {\"action\": \"dropped\", \"count\": 1}\n",
    "            print(f\"  已删除重复列: {col}\")\n",
    "    \n",
    "    # 3. 统一时间格式\n",
    "    # 定义需要处理的时间列\n",
    "    time_cols = ['created_at', 'updated_at', 'closed_at', 'merged_at', 'last_comment_time']\n",
    "    \n",
    "    for col in time_cols:\n",
    "        if col in df.columns:\n",
    "            # 初始化列统计\n",
    "            if col not in stats[\"columns_processed\"]:\n",
    "                stats[\"columns_processed\"][col] = {\"filled\": 0, \"removed\": 0}\n",
    "                \n",
    "            # 记录原始缺失值数量\n",
    "            original_missing = df[col].isna().sum()\n",
    "            \n",
    "            # 尝试转换为datetime格式\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            except:\n",
    "                # 如果转换失败，尝试修复格式\n",
    "                df[col] = df[col].apply(lambda x: fix_time_format(x) if pd.notnull(x) else x)\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            \n",
    "            # 记录转换后的缺失值\n",
    "            new_missing = df[col].isna().sum()\n",
    "            if new_missing > original_missing:\n",
    "                stats[\"columns_processed\"][col][\"filled\"] += (new_missing - original_missing)\n",
    "                stats[\"filled_values\"] += (new_missing - original_missing)\n",
    "    \n",
    "    # 4. 处理 closed_at 缺失值 - 只处理非open状态的PR\n",
    "    if 'closed_at' in df.columns and 'state' in df.columns:\n",
    "        if 'closed_at' not in stats[\"columns_processed\"]:\n",
    "            stats[\"columns_processed\"][\"closed_at\"] = {\"filled\": 0, \"removed\": 0}\n",
    "            \n",
    "        # 只处理非open状态的PR\n",
    "        non_open_mask = df['state'] != 'open'\n",
    "        \n",
    "        # 记录原始缺失值数量\n",
    "        original_missing = df.loc[non_open_mask, 'closed_at'].isna().sum()\n",
    "        \n",
    "        # 用 updated_at 填充 closed_at（仅限非open状态）\n",
    "        mask = non_open_mask & df['closed_at'].isna() & df['updated_at'].notna()\n",
    "        filled_count = mask.sum()\n",
    "        df.loc[mask, 'closed_at'] = df.loc[mask, 'updated_at']\n",
    "        \n",
    "        # 记录填充数量\n",
    "        stats[\"columns_processed\"][\"closed_at\"][\"filled\"] += filled_count\n",
    "        stats[\"filled_values\"] += filled_count\n",
    "        \n",
    "        # 删除仍缺失的行（仅限非open状态）\n",
    "        mask_to_drop = non_open_mask & df['closed_at'].isna()\n",
    "        initial_count = len(df)\n",
    "        df = df[~mask_to_drop]\n",
    "        removed_count = initial_count - len(df)\n",
    "        \n",
    "        # 记录删除数量\n",
    "        stats[\"columns_processed\"][\"closed_at\"][\"removed\"] += removed_count\n",
    "        stats[\"removed_values\"] += removed_count\n",
    "        stats[\"rows_removed\"] += removed_count\n",
    "    \n",
    "    # 5. 处理其他缺失值\n",
    "    high_missing_cols = {\n",
    "        'merged_at': pd.NaT,\n",
    "        'avg_comment_length': 0,\n",
    "        'comment_count': 0,\n",
    "        'name_reviewer': 'Unknown',\n",
    "        'last_comment_time': df['created_at'],\n",
    "        'modify_entropy': df['modify_entropy'].median(),\n",
    "        'body': 'null',  # 修改为'null'\n",
    "        'title': 'null'   # 添加title字段的填充\n",
    "    }\n",
    "    \n",
    "    for col, fill_value in high_missing_cols.items():\n",
    "        if col in df.columns:\n",
    "            # 初始化列统计\n",
    "            if col not in stats[\"columns_processed\"]:\n",
    "                stats[\"columns_processed\"][col] = {\"filled\": 0, \"removed\": 0}\n",
    "                \n",
    "            # 记录原始缺失值数量\n",
    "            original_missing = df[col].isna().sum()\n",
    "            \n",
    "            # 填充缺失值\n",
    "            if isinstance(fill_value, pd.Series):\n",
    "                df[col] = df[col].fillna(fill_value)\n",
    "            else:\n",
    "                df[col] = df[col].fillna(fill_value)\n",
    "            \n",
    "            # 记录填充数量\n",
    "            filled_count = original_missing - df[col].isna().sum()\n",
    "            stats[\"columns_processed\"][col][\"filled\"] += filled_count\n",
    "            stats[\"filled_values\"] += filled_count\n",
    "    \n",
    "    # 其他列处理\n",
    "    for col in df.columns:\n",
    "        if col in stats[\"columns_processed\"]:\n",
    "            continue  # 已处理过的列跳过\n",
    "            \n",
    "        if df[col].isna().sum() > 0:\n",
    "            # 初始化列统计\n",
    "            stats[\"columns_processed\"][col] = {\"filled\": 0, \"removed\": 0}\n",
    "            \n",
    "            # 记录原始缺失值数量\n",
    "            original_missing = df[col].isna().sum()\n",
    "            \n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                # 数值列用中位数填充\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                # 时间列不填充，保留缺失值\n",
    "                pass  # 不填充，保留NaN\n",
    "            else:\n",
    "                # 分类列用众数填充\n",
    "                mode_value = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
    "                df[col] = df[col].fillna(mode_value)\n",
    "            \n",
    "            # 记录填充数量\n",
    "            filled_count = original_missing - df[col].isna().sum()\n",
    "            stats[\"columns_processed\"][col][\"filled\"] += filled_count\n",
    "            stats[\"filled_values\"] += filled_count\n",
    "    \n",
    "    # 6. 处理异常值\n",
    "    # 数值异常处理\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    for col in numeric_cols:\n",
    "        # 负值设为0\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        df[col] = np.where(df[col] < 0, 0, df[col])\n",
    "        \n",
    "        # 记录处理数量\n",
    "        if negative_count > 0:\n",
    "            if col not in stats[\"columns_processed\"]:\n",
    "                stats[\"columns_processed\"][col] = {\"filled\": 0, \"removed\": 0}\n",
    "            stats[\"columns_processed\"][col][\"filled\"] += negative_count\n",
    "            stats[\"filled_values\"] += negative_count\n",
    "        \n",
    "        # 处理极端值（Winsorize）\n",
    "        q1 = df[col].quantile(0.01)\n",
    "        q99 = df[col].quantile(0.99)\n",
    "        low_outliers = (df[col] < q1).sum()\n",
    "        high_outliers = (df[col] > q99).sum()\n",
    "        \n",
    "        df[col] = np.where(df[col] < q1, q1, df[col])\n",
    "        df[col] = np.where(df[col] > q99, q99, df[col])\n",
    "        \n",
    "        # 记录处理数量\n",
    "        if low_outliers > 0 or high_outliers > 0:\n",
    "            if col not in stats[\"columns_processed\"]:\n",
    "                stats[\"columns_processed\"][col] = {\"filled\": 0, \"removed\": 0}\n",
    "            stats[\"columns_processed\"][col][\"filled\"] += (low_outliers + high_outliers)\n",
    "            stats[\"filled_values\"] += (low_outliers + high_outliers)\n",
    "    \n",
    "    # 7. 数据类型转换\n",
    "    # 扩展布尔列列表\n",
    "    bool_cols = [\n",
    "        'merged', 'has_test', 'has_feature', 'has_bug', \n",
    "        'has_document', 'has_improve', 'has_refactor', \n",
    "        'is_reviewed', 'last_comment_mention', 'is_reviewer', 'is_author'\n",
    "    ]\n",
    "    \n",
    "    for col in bool_cols:\n",
    "        if col in df.columns:\n",
    "            # 记录原始非数值数量\n",
    "            non_numeric_count = (~df[col].apply(lambda x: isinstance(x, (int, float)))).sum()\n",
    "            \n",
    "            df[col] = df[col].astype(int)\n",
    "            \n",
    "            # 记录转换数量\n",
    "            if non_numeric_count > 0:\n",
    "                if col not in stats[\"columns_processed\"]:\n",
    "                    stats[\"columns_processed\"][col] = {\"filled\": 0, \"removed\": 0}\n",
    "                stats[\"columns_processed\"][col][\"filled\"] += non_numeric_count\n",
    "                stats[\"filled_values\"] += non_numeric_count\n",
    "    \n",
    "    # 8. 文本清洗\n",
    "    if 'body' in df.columns:\n",
    "        # 记录原始非字符串数量\n",
    "        non_string_count = (~df['body'].apply(lambda x: isinstance(x, str))).sum()\n",
    "        \n",
    "        df['body'] = df['body'].apply(clean_text)\n",
    "        \n",
    "        # 确保body字段没有缺失值（最终检查）\n",
    "        df['body'] = df['body'].fillna('null')  # 修改为'null'\n",
    "        \n",
    "        # 记录处理数量\n",
    "        if non_string_count > 0:\n",
    "            if 'body' not in stats[\"columns_processed\"]:\n",
    "                stats[\"columns_processed\"]['body'] = {\"filled\": 0, \"removed\": 0}\n",
    "            stats[\"columns_processed\"]['body'][\"filled\"] += non_string_count\n",
    "            stats[\"filled_values\"] += non_string_count\n",
    "    \n",
    "    if 'title' in df.columns:\n",
    "        # 记录原始非字符串数量\n",
    "        non_string_count = (~df['title'].apply(lambda x: isinstance(x, str))).sum()\n",
    "        \n",
    "        df['title'] = df['title'].apply(clean_text)\n",
    "        \n",
    "        # 确保title字段没有缺失值（最终检查）\n",
    "        df['title'] = df['title'].fillna('null')  # 修改为'null'\n",
    "        \n",
    "        # 记录处理数量\n",
    "        if non_string_count > 0:\n",
    "            if 'title' not in stats[\"columns_processed\"]:\n",
    "                stats[\"columns_processed\"]['title'] = {\"filled\": 0, \"removed\": 0}\n",
    "            stats[\"columns_processed\"]['title'][\"filled\"] += non_string_count\n",
    "            stats[\"filled_values\"] += non_string_count\n",
    "    \n",
    "    # 添加最终行数\n",
    "    stats[\"final_rows\"] = len(df)\n",
    "    \n",
    "    # 移除时间列的时区信息（解决Excel不支持时区的问题）\n",
    "    for col in time_cols:\n",
    "        if col in df.columns:\n",
    "            # 使用新的方法检查时区类型\n",
    "            if isinstance(df[col].dtype, pd.DatetimeTZDtype):\n",
    "                df[col] = df[col].dt.tz_localize(None)\n",
    "            # 或者转换为无时区的datetime\n",
    "            elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                df[col] = pd.to_datetime(df[col]).dt.tz_localize(None)\n",
    "    \n",
    "    return df, stats\n",
    "\n",
    "def fix_time_format(time_str):\n",
    "    \"\"\"\n",
    "    修复时间格式问题\n",
    "    \"\"\"\n",
    "    if pd.isnull(time_str):\n",
    "        return time_str\n",
    "    \n",
    "    # 尝试解析常见格式\n",
    "    try:\n",
    "        # ISO 8601格式 (YYYY-MM-DDTHH:MM:SSZ)\n",
    "        if 'T' in time_str and 'Z' in time_str:\n",
    "            return pd.to_datetime(time_str, utc=True)\n",
    "        \n",
    "        # 日期时间格式 (YYYY-MM-DD HH:MM:SS)\n",
    "        if ' ' in time_str and len(time_str) > 10:\n",
    "            return pd.to_datetime(time_str)\n",
    "        \n",
    "        # 纯日期格式 (YYYY-MM-DD)\n",
    "        if len(time_str) == 10 and '-' in time_str:\n",
    "            return pd.to_datetime(time_str)\n",
    "        \n",
    "        # 其他格式尝试解析\n",
    "        return pd.to_datetime(time_str)\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    清洗文本数据\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return \"null\"  # 修改为'null'\n",
    "    \n",
    "    # 确保是字符串\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # 移除HTML标签\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    # 移除特殊字符\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:\\'\"-]', '', text)\n",
    "    # 截断长文本\n",
    "    return text[:500]\n",
    "\n",
    "# 自定义JSON编码器，处理numpy类型\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, pd.Timestamp):\n",
    "            return str(obj)\n",
    "        else:\n",
    "            return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建清洗结果目录\n",
    "    cleaned_dir = \"processed_dataset/cleaned\"\n",
    "    stats_dir = \"processed_dataset/stats\"\n",
    "    os.makedirs(cleaned_dir, exist_ok=True)\n",
    "    os.makedirs(stats_dir, exist_ok=True)\n",
    "    \n",
    "    # 获取所有项目列表\n",
    "    dataset_dir = \"dataset\"\n",
    "    projects = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "    \n",
    "    print(f\"开始清洗所有项目数据，共 {len(projects)} 个项目\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 初始化总统计\n",
    "    total_stats = {\n",
    "        \"projects\": [],\n",
    "        \"total_original_rows\": 0,\n",
    "        \"total_final_rows\": 0,\n",
    "        \"total_rows_removed\": 0,\n",
    "        \"total_filled_values\": 0,\n",
    "        \"total_removed_values\": 0\n",
    "    }\n",
    "    \n",
    "    # 遍历所有项目\n",
    "    for project in tqdm(projects, desc=\"清洗项目\"):\n",
    "        merged_file = os.path.join(\"processed_dataset\", \"merged\", f\"{project}_merged.xlsx\")\n",
    "        cleaned_file = os.path.join(cleaned_dir, f\"{project}_cleaned.xlsx\")\n",
    "        stats_file = os.path.join(stats_dir, f\"{project}_stats.json\")\n",
    "        \n",
    "        print(f\"\\n清洗项目: {project}\")\n",
    "        print(f\"  输入文件: {merged_file}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取合并后的数据\n",
    "            df = pd.read_excel(merged_file)\n",
    "            print(f\"  原始记录数: {len(df)}\")\n",
    "            print(f\"  原始列数: {len(df.columns)}\")\n",
    "            \n",
    "            # 更新总统计\n",
    "            total_stats[\"total_original_rows\"] += len(df)\n",
    "            \n",
    "            # 清洗数据\n",
    "            df_cleaned, stats = clean_project_data(df)\n",
    "            print(f\"  清洗后记录数: {len(df_cleaned)}\")\n",
    "            print(f\"  清洗后列数: {len(df_cleaned.columns)}\")\n",
    "            \n",
    "            # 更新总统计\n",
    "            total_stats[\"projects\"].append(project)\n",
    "            total_stats[\"total_final_rows\"] += len(df_cleaned)\n",
    "            total_stats[\"total_rows_removed\"] += stats[\"rows_removed\"]\n",
    "            total_stats[\"total_filled_values\"] += stats[\"filled_values\"]\n",
    "            total_stats[\"total_removed_values\"] += stats[\"removed_values\"]\n",
    "            \n",
    "            # 保存清洗后的数据\n",
    "            # 确保所有时间列都是无时区的\n",
    "            time_cols = [\n",
    "                'created_at', 'updated_at', 'closed_at', 'merged_at', 'last_comment_time'\n",
    "            ]\n",
    "            for col in time_cols:\n",
    "                if col in df_cleaned.columns and pd.api.types.is_datetime64_any_dtype(df_cleaned[col]):\n",
    "                    df_cleaned[col] = df_cleaned[col].dt.tz_localize(None)\n",
    "            \n",
    "            # 确保body和title字段没有缺失值\n",
    "            if 'body' in df_cleaned.columns:\n",
    "                df_cleaned['body'] = df_cleaned['body'].fillna('null')\n",
    "            if 'title' in df_cleaned.columns:\n",
    "                df_cleaned['title'] = df_cleaned['title'].fillna('null')\n",
    "            \n",
    "            df_cleaned.to_excel(cleaned_file, index=False)\n",
    "            print(f\"  已保存到: {cleaned_file}\")\n",
    "            \n",
    "            # 保存统计信息 - 使用自定义编码器处理numpy类型\n",
    "            with open(stats_file, 'w') as f:\n",
    "                json.dump(stats, f, indent=4, cls=NumpyEncoder)\n",
    "            print(f\"  统计信息已保存到: {stats_file}\")\n",
    "            \n",
    "            # 打印详细统计\n",
    "            print(f\"  删除行数: {stats['rows_removed']}\")\n",
    "            print(f\"  填充值数: {stats['filled_values']}\")\n",
    "            print(f\"  删除值数: {stats['removed_values']}\")\n",
    "            print(f\"  删除列数: {sum(1 for col_stats in stats['columns_processed'].values() if col_stats.get('action') == 'dropped')}\")\n",
    "            print(\"-\"*50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  清洗失败: {str(e)}\")\n",
    "            print(\"-\"*50)\n",
    "    \n",
    "    # 保存总统计 - 使用自定义编码器处理numpy类型\n",
    "    total_stats_file = os.path.join(stats_dir, \"total_stats.json\")\n",
    "    with open(total_stats_file, 'w') as f:\n",
    "        json.dump(total_stats, f, indent=4, cls=NumpyEncoder)\n",
    "    \n",
    "    print(\"\\n所有项目数据清洗完成!\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"总原始行数: {total_stats['total_original_rows']}\")\n",
    "    print(f\"总最终行数: {total_stats['total_final_rows']}\")\n",
    "    print(f\"总删除行数: {total_stats['total_rows_removed']}\")\n",
    "    print(f\"总填充值数: {total_stats['total_filled_values']}\")\n",
    "    print(f\"总删除值数: {total_stats['total_removed_values']}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"总统计信息已保存到: {total_stats_file}\")"
   ],
   "id": "715b700c3599714b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始清洗所有项目数据，共 10 个项目\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清洗项目:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "清洗项目: django\n",
      "  输入文件: processed_dataset\\merged\\django_merged.xlsx\n",
      "  原始记录数: 16976\n",
      "  原始列数: 115\n",
      "  已删除重复列: created_at_time\n",
      "  已删除重复列: updated_at_time\n",
      "  已删除重复列: closed_at_time\n",
      "  已删除重复列: merged_at_time\n",
      "  清洗后记录数: 16976\n",
      "  清洗后列数: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清洗项目:  10%|█         | 1/10 [01:04<09:38, 64.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/cleaned\\django_cleaned.xlsx\n",
      "  统计信息已保存到: processed_dataset/stats\\django_stats.json\n",
      "  删除行数: 0\n",
      "  填充值数: 30566\n",
      "  删除值数: 0\n",
      "  删除列数: 4\n",
      "--------------------------------------------------\n",
      "\n",
      "清洗项目: moby\n",
      "  输入文件: processed_dataset\\merged\\moby_merged.xlsx\n",
      "  原始记录数: 23515\n",
      "  原始列数: 115\n",
      "  已删除重复列: created_at_time\n",
      "  已删除重复列: updated_at_time\n",
      "  已删除重复列: closed_at_time\n",
      "  已删除重复列: merged_at_time\n",
      "  清洗后记录数: 23515\n",
      "  清洗后列数: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清洗项目:  20%|██        | 2/10 [02:33<10:32, 79.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/cleaned\\moby_cleaned.xlsx\n",
      "  统计信息已保存到: processed_dataset/stats\\moby_stats.json\n",
      "  删除行数: 0\n",
      "  填充值数: 24448\n",
      "  删除值数: 0\n",
      "  删除列数: 4\n",
      "--------------------------------------------------\n",
      "\n",
      "清洗项目: opencv\n",
      "  输入文件: processed_dataset\\merged\\opencv_merged.xlsx\n",
      "  原始记录数: 13972\n",
      "  原始列数: 115\n",
      "  已删除重复列: created_at_time\n",
      "  已删除重复列: updated_at_time\n",
      "  已删除重复列: closed_at_time\n",
      "  已删除重复列: merged_at_time\n",
      "  清洗后记录数: 13972\n",
      "  清洗后列数: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清洗项目:  30%|███       | 3/10 [03:25<07:46, 66.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/cleaned\\opencv_cleaned.xlsx\n",
      "  统计信息已保存到: processed_dataset/stats\\opencv_stats.json\n",
      "  删除行数: 0\n",
      "  填充值数: 22604\n",
      "  删除值数: 0\n",
      "  删除列数: 4\n",
      "--------------------------------------------------\n",
      "\n",
      "清洗项目: react\n",
      "  输入文件: processed_dataset\\merged\\react_merged.xlsx\n",
      "  原始记录数: 13671\n",
      "  原始列数: 115\n",
      "  已删除重复列: created_at_time\n",
      "  已删除重复列: updated_at_time\n",
      "  已删除重复列: closed_at_time\n",
      "  已删除重复列: merged_at_time\n",
      "  清洗后记录数: 13671\n",
      "  清洗后列数: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清洗项目:  40%|████      | 4/10 [04:16<06:03, 60.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/cleaned\\react_cleaned.xlsx\n",
      "  统计信息已保存到: processed_dataset/stats\\react_stats.json\n",
      "  删除行数: 0\n",
      "  填充值数: 17278\n",
      "  删除值数: 0\n",
      "  删除列数: 4\n",
      "--------------------------------------------------\n",
      "\n",
      "清洗项目: salt\n",
      "  输入文件: processed_dataset\\merged\\salt_merged.xlsx\n",
      "  原始记录数: 39025\n",
      "  原始列数: 115\n",
      "  已删除重复列: created_at_time\n",
      "  已删除重复列: updated_at_time\n",
      "  已删除重复列: closed_at_time\n",
      "  已删除重复列: merged_at_time\n",
      "  清洗后记录数: 39025\n",
      "  清洗后列数: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清洗项目:  50%|█████     | 5/10 [06:37<07:26, 89.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/cleaned\\salt_cleaned.xlsx\n",
      "  统计信息已保存到: processed_dataset/stats\\salt_stats.json\n",
      "  删除行数: 0\n",
      "  填充值数: 96278\n",
      "  删除值数: 0\n",
      "  删除列数: 4\n",
      "--------------------------------------------------\n",
      "\n",
      "清洗项目: scikit-learn\n",
      "  输入文件: processed_dataset\\merged\\scikit-learn_merged.xlsx\n",
      "  原始记录数: 15687\n",
      "  原始列数: 115\n",
      "  已删除重复列: created_at_time\n",
      "  已删除重复列: updated_at_time\n",
      "  已删除重复列: closed_at_time\n",
      "  已删除重复列: merged_at_time\n",
      "  清洗后记录数: 15687\n",
      "  清洗后列数: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清洗项目:  60%|██████    | 6/10 [07:39<05:21, 80.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/cleaned\\scikit-learn_cleaned.xlsx\n",
      "  统计信息已保存到: processed_dataset/stats\\scikit-learn_stats.json\n",
      "  删除行数: 0\n",
      "  填充值数: 24465\n",
      "  删除值数: 0\n",
      "  删除列数: 4\n",
      "--------------------------------------------------\n",
      "\n",
      "清洗项目: symfony\n",
      "  输入文件: processed_dataset\\merged\\symfony_merged.xlsx\n",
      "  原始记录数: 30392\n",
      "  原始列数: 115\n",
      "  已删除重复列: created_at_time\n",
      "  已删除重复列: updated_at_time\n",
      "  已删除重复列: closed_at_time\n",
      "  已删除重复列: merged_at_time\n",
      "  清洗后记录数: 30392\n",
      "  清洗后列数: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清洗项目:  70%|███████   | 7/10 [09:35<04:35, 91.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/cleaned\\symfony_cleaned.xlsx\n",
      "  统计信息已保存到: processed_dataset/stats\\symfony_stats.json\n",
      "  删除行数: 0\n",
      "  填充值数: 42881\n",
      "  删除值数: 0\n",
      "  删除列数: 4\n",
      "--------------------------------------------------\n",
      "\n",
      "清洗项目: tensorflow\n",
      "  输入文件: processed_dataset\\merged\\tensorflow_merged.xlsx\n",
      "  原始记录数: 23130\n",
      "  原始列数: 115\n",
      "  已删除重复列: created_at_time\n",
      "  已删除重复列: updated_at_time\n",
      "  已删除重复列: closed_at_time\n",
      "  已删除重复列: merged_at_time\n",
      "  清洗后记录数: 23130\n",
      "  清洗后列数: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清洗项目:  80%|████████  | 8/10 [10:59<02:58, 89.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/cleaned\\tensorflow_cleaned.xlsx\n",
      "  统计信息已保存到: processed_dataset/stats\\tensorflow_stats.json\n",
      "  删除行数: 0\n",
      "  填充值数: 52326\n",
      "  删除值数: 0\n",
      "  删除列数: 4\n",
      "--------------------------------------------------\n",
      "\n",
      "清洗项目: terraform\n",
      "  输入文件: processed_dataset\\merged\\terraform_merged.xlsx\n",
      "  原始记录数: 13219\n",
      "  原始列数: 115\n",
      "  已删除重复列: created_at_time\n",
      "  已删除重复列: updated_at_time\n",
      "  已删除重复列: closed_at_time\n",
      "  已删除重复列: merged_at_time\n",
      "  清洗后记录数: 13219\n",
      "  清洗后列数: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清洗项目:  90%|█████████ | 9/10 [11:49<01:16, 76.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/cleaned\\terraform_cleaned.xlsx\n",
      "  统计信息已保存到: processed_dataset/stats\\terraform_stats.json\n",
      "  删除行数: 0\n",
      "  填充值数: 11493\n",
      "  删除值数: 0\n",
      "  删除列数: 4\n",
      "--------------------------------------------------\n",
      "\n",
      "清洗项目: yii2\n",
      "  输入文件: processed_dataset\\merged\\yii2_merged.xlsx\n",
      "  原始记录数: 8040\n",
      "  原始列数: 115\n",
      "  已删除重复列: created_at_time\n",
      "  已删除重复列: updated_at_time\n",
      "  已删除重复列: closed_at_time\n",
      "  已删除重复列: merged_at_time\n",
      "  清洗后记录数: 8040\n",
      "  清洗后列数: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "清洗项目: 100%|██████████| 10/10 [12:19<00:00, 73.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/cleaned\\yii2_cleaned.xlsx\n",
      "  统计信息已保存到: processed_dataset/stats\\yii2_stats.json\n",
      "  删除行数: 0\n",
      "  填充值数: 11115\n",
      "  删除值数: 0\n",
      "  删除列数: 4\n",
      "--------------------------------------------------\n",
      "\n",
      "所有项目数据清洗完成!\n",
      "==================================================\n",
      "总原始行数: 197627\n",
      "总最终行数: 197627\n",
      "总删除行数: 0\n",
      "总填充值数: 333454\n",
      "总删除值数: 0\n",
      "==================================================\n",
      "总统计信息已保存到: processed_dataset/stats\\total_stats.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 第三步 特征工程\n",
    "\n",
    "### 时间特征：\n",
    "\n",
    "processing_time：PR从创建到关闭的总时间（小时）\n",
    "\n",
    "first_response_time：首次评论响应时间（小时）\n",
    "\n",
    "last_response_time：最后评论响应时间（小时）\n",
    "\n",
    "created_hour：创建时间的小时（0-23）\n",
    "\n",
    "created_dayofweek：创建时间的星期几（0=周一，6=周日）\n",
    "\n",
    "created_month：创建时间的月份（1-12）\n",
    "\n",
    "### 文本特征：\n",
    "\n",
    "title_length：标题长度（字符数）\n",
    "\n",
    "body_length：正文长度（字符数）\n",
    "\n",
    "has_bug_keyword：是否包含bug相关关键词\n",
    "\n",
    "has_feature_keyword：是否包含功能相关关键词\n",
    "\n",
    "has_document_keyword：是否包含文档相关关键词\n",
    "\n",
    "### 变更特征：\n",
    "\n",
    "total_changes：总变更行数（添加+删除）\n",
    "\n",
    "net_changes：净变更行数（添加-删除）\n",
    "\n",
    "change_density：变更密度（每文件变更行数）\n",
    "\n",
    "additions_per_file：每文件添加行数\n",
    "\n",
    "### 参与者特征：\n",
    "\n",
    "author_experience：作者经验（项目内首次提交到当前PR的天数）\n",
    "\n",
    "author_activity：作者活跃度（项目内PR数量）\n",
    "\n",
    "reviewer_count：评审者数量（估算）\n",
    "\n",
    "### 项目特征：\n",
    "\n",
    "project_age：项目年龄（从第一个PR到当前PR的天数）\n",
    "\n",
    "open_pr_count：开放PR数量（项目内）\n",
    "\n",
    "### 交互特征：\n",
    "\n",
    "author_exp_change_size：作者经验×变更大小\n",
    "\n",
    "complexity_per_reviewer：每个评审者的变更复杂度"
   ],
   "id": "b61a375162ac99d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T01:26:13.304165Z",
     "start_time": "2025-10-16T01:11:36.012498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# 创建特征工程目录\n",
    "engineered_dir = \"processed_dataset/engineered\"\n",
    "os.makedirs(engineered_dir, exist_ok=True)\n",
    "\n",
    "# 获取所有项目列表\n",
    "cleaned_dir = \"processed_dataset/cleaned\"\n",
    "projects = [f.replace('_cleaned.xlsx', '') for f in os.listdir(cleaned_dir) if f.endswith('_cleaned.xlsx')]\n",
    "\n",
    "print(f\"开始特征工程，共 {len(projects)} 个项目\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 关键词列表\n",
    "bug_keywords = ['bug', 'fix', 'error', 'issue', 'defect', 'fault']\n",
    "feature_keywords = ['feature', 'enhance', 'new', 'improve', 'add', 'implement']\n",
    "document_keywords = ['doc', 'readme', 'document', 'comment', 'note']\n",
    "\n",
    "def extract_features(df):\n",
    "    \"\"\"\n",
    "    从清洗后的数据中提取特征\n",
    "    \"\"\"\n",
    "    # 记录原始列数\n",
    "    original_cols = set(df.columns)\n",
    "    \n",
    "    # 打印原始列名用于调试\n",
    "    print(f\"  原始列: {list(original_cols)}\")\n",
    "    \n",
    "    # 1. 时间特征\n",
    "    # 确保时间列是datetime类型\n",
    "    time_cols = ['created_at', 'closed_at', 'last_comment_time']\n",
    "    for col in time_cols:\n",
    "        if col in df.columns:\n",
    "            print(f\"  处理时间列: {col}\")\n",
    "            # 确保列存在且不为空\n",
    "            if df[col].isna().any():\n",
    "                # 填充缺失值为当前时间\n",
    "                df[col] = df[col].fillna(pd.Timestamp.now())\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "                print(f\"    成功转换为datetime类型\")\n",
    "            except Exception as e:\n",
    "                print(f\"    转换失败: {str(e)}\")\n",
    "    \n",
    "    # 时间差特征 - 处理缺失值\n",
    "    if 'closed_at' in df.columns and 'created_at' in df.columns:\n",
    "        print(\"  计算处理时间\")\n",
    "        # 确保没有缺失值\n",
    "        mask = df['closed_at'].notna() & df['created_at'].notna()\n",
    "        df.loc[mask, 'processing_time'] = (df.loc[mask, 'closed_at'] - df.loc[mask, 'created_at']).dt.total_seconds() / 3600\n",
    "        # 填充缺失值\n",
    "        df['processing_time'] = df['processing_time'].fillna(-1)  # 用-1表示缺失\n",
    "        print(f\"    添加特征: processing_time\")\n",
    "    \n",
    "    if 'last_comment_time' in df.columns and 'created_at' in df.columns:\n",
    "        print(\"  计算最后响应时间\")\n",
    "        mask = df['last_comment_time'].notna() & df['created_at'].notna()\n",
    "        df.loc[mask, 'last_response_time'] = (df.loc[mask, 'last_comment_time'] - df.loc[mask, 'created_at']).dt.total_seconds() / 3600\n",
    "        df['last_response_time'] = df['last_response_time'].fillna(-1)\n",
    "        print(f\"    添加特征: last_response_time\")\n",
    "    \n",
    "    # 时间分解特征 - 处理缺失值\n",
    "    if 'created_at' in df.columns:\n",
    "        print(\"  分解创建时间\")\n",
    "        # 确保没有缺失值\n",
    "        df['created_at'] = df['created_at'].fillna(pd.Timestamp.now())\n",
    "        df['created_hour'] = df['created_at'].dt.hour\n",
    "        df['created_dayofweek'] = df['created_at'].dt.dayofweek  # 0=周一, 6=周日\n",
    "        df['created_month'] = df['created_at'].dt.month\n",
    "        print(f\"    添加特征: created_hour, created_dayofweek, created_month\")\n",
    "    \n",
    "    # 2. 文本特征 - 处理缺失值\n",
    "    if 'title' in df.columns:\n",
    "        print(\"  处理标题特征\")\n",
    "        df['title'] = df['title'].fillna('null')\n",
    "        df['title_length'] = df['title'].str.len()\n",
    "        print(f\"    添加特征: title_length\")\n",
    "    \n",
    "    if 'body' in df.columns:\n",
    "        print(\"  处理正文特征\")\n",
    "        df['body'] = df['body'].fillna('null')\n",
    "        df['body_length'] = df['body'].str.len()\n",
    "        print(f\"    添加特征: body_length\")\n",
    "        \n",
    "        # 转换为小写以便匹配\n",
    "        body_lower = df['body'].str.lower().fillna('null')\n",
    "        \n",
    "        # 定义关键词列表\n",
    "        bug_keywords = ['bug', 'fix', 'error', 'issue', 'defect', 'fault']\n",
    "        feature_keywords = ['feature', 'enhance', 'new', 'improve', 'add', 'implement']\n",
    "        document_keywords = ['doc', 'readme', 'document', 'comment', 'note']\n",
    "        \n",
    "        # 关键词特征 - 处理缺失值\n",
    "        print(\"  添加关键词特征\")\n",
    "        for keyword_list, col_name in zip([bug_keywords, feature_keywords, document_keywords], \n",
    "                                         ['has_bug_keyword', 'has_feature_keyword', 'has_document_keyword']):\n",
    "            # 使用apply安全处理\n",
    "            df[col_name] = body_lower.apply(\n",
    "                lambda x: 1 if any(kw in x for kw in keyword_list) else 0\n",
    "            )\n",
    "            print(f\"    添加特征: {col_name}\")\n",
    "    \n",
    "    # 3. 变更特征 - 处理缺失值\n",
    "    if 'additions' in df.columns and 'deletions' in df.columns:\n",
    "        print(\"  处理变更特征\")\n",
    "        # 填充缺失值\n",
    "        df['additions'] = df['additions'].fillna(0)\n",
    "        df['deletions'] = df['deletions'].fillna(0)\n",
    "        df['total_changes'] = df['additions'] + df['deletions']\n",
    "        df['net_changes'] = df['additions'] - df['deletions']\n",
    "        print(f\"    添加特征: total_changes, net_changes\")\n",
    "    \n",
    "    if 'changed_files' in df.columns:\n",
    "        print(\"  处理文件变更特征\")\n",
    "        # 填充缺失值并避免除以0\n",
    "        df['changed_files'] = df['changed_files'].fillna(1)\n",
    "        changed_files = df['changed_files'].replace(0, 1)\n",
    "        \n",
    "        if 'total_changes' in df.columns:\n",
    "            df['change_density'] = df['total_changes'] / changed_files\n",
    "            print(f\"    添加特征: change_density\")\n",
    "        \n",
    "        if 'additions' in df.columns:\n",
    "            df['additions_per_file'] = df['additions'] / changed_files\n",
    "            print(f\"    添加特征: additions_per_file\")\n",
    "    \n",
    "    # 4. 参与者特征 - 处理缺失值\n",
    "    if 'author' in df.columns:\n",
    "        print(\"  处理作者特征\")\n",
    "        # 确保作者列没有缺失值\n",
    "        df['author'] = df['author'].fillna('Unknown')\n",
    "        \n",
    "        # 作者经验（项目内首次提交到当前PR的天数）\n",
    "        author_min_date = df.groupby('author')['created_at'].transform('min')\n",
    "        df['author_experience'] = (df['created_at'] - author_min_date).dt.days.fillna(0)\n",
    "        print(f\"    添加特征: author_experience\")\n",
    "        \n",
    "        # 作者活跃度（项目内PR数量）\n",
    "        df['author_activity'] = df.groupby('author')['number'].transform('count').fillna(0)\n",
    "        print(f\"    添加特征: author_activity\")\n",
    "    \n",
    "    # 使用'review_comments'列估算评审者数量\n",
    "    if 'review_comments' in df.columns:\n",
    "        print(\"  处理评审者特征\")\n",
    "        # 简单估算评审者数量（每个评论视为一个评审者）\n",
    "        # 安全处理非列表类型\n",
    "        df['reviewer_count'] = df['review_comments'].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 1 if pd.notna(x) else 0\n",
    "        )\n",
    "        print(f\"    添加特征: reviewer_count\")\n",
    "    \n",
    "    # 5. 项目特征 - 处理缺失值\n",
    "    if 'project' in df.columns and 'created_at' in df.columns:\n",
    "        print(\"  处理项目特征\")\n",
    "        # 确保项目列没有缺失值\n",
    "        df['project'] = df['project'].fillna('Unknown')\n",
    "        \n",
    "        # 项目年龄（从第一个PR到当前PR的天数）\n",
    "        project_min_date = df.groupby('project')['created_at'].transform('min')\n",
    "        df['project_age'] = (df['created_at'] - project_min_date).dt.days.fillna(0)\n",
    "        print(f\"    添加特征: project_age\")\n",
    "        \n",
    "        # 开放PR数量（项目内）\n",
    "        open_pr_count = df.groupby('project')['state'].transform(lambda x: (x == 'open').sum())\n",
    "        df['open_pr_count'] = open_pr_count.fillna(0)\n",
    "        print(f\"    添加特征: open_pr_count\")\n",
    "    \n",
    "    # 6. 交互特征 - 处理缺失值\n",
    "    if 'author_experience' in df.columns and 'total_changes' in df.columns:\n",
    "        print(\"  添加交互特征\")\n",
    "        # 填充缺失值\n",
    "        df['author_experience'] = df['author_experience'].fillna(0)\n",
    "        df['total_changes'] = df['total_changes'].fillna(0)\n",
    "        df['author_exp_change_size'] = df['author_experience'] * df['total_changes']\n",
    "        print(f\"    添加特征: author_exp_change_size\")\n",
    "    \n",
    "    if 'reviewer_count' in df.columns and 'total_changes' in df.columns:\n",
    "        # 填充缺失值\n",
    "        df['reviewer_count'] = df['reviewer_count'].fillna(0)\n",
    "        df['total_changes'] = df['total_changes'].fillna(0)\n",
    "        df['complexity_per_reviewer'] = df['total_changes'] / (df['reviewer_count'].replace(0, 1))  # 避免除以0\n",
    "        print(f\"    添加特征: complexity_per_reviewer\")\n",
    "    \n",
    "    # 计算新增特征数\n",
    "    new_cols = set(df.columns) - original_cols\n",
    "    print(f\"  新增特征数: {len(new_cols)}\")\n",
    "    print(f\"  新增特征列表: {list(new_cols)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 遍历所有项目\n",
    "for project in tqdm(projects, desc=\"特征工程\"):\n",
    "    cleaned_file = os.path.join(cleaned_dir, f\"{project}_cleaned.xlsx\")\n",
    "    engineered_file = os.path.join(engineered_dir, f\"{project}_engineered.xlsx\")\n",
    "    \n",
    "    print(f\"\\n处理项目: {project}\")\n",
    "    print(f\"  输入文件: {cleaned_file}\")\n",
    "    \n",
    "    try:\n",
    "        # 读取清洗后的数据\n",
    "        df = pd.read_excel(cleaned_file)\n",
    "        print(f\"  原始记录数: {len(df)}\")\n",
    "        \n",
    "        # 特征工程\n",
    "        df_engineered = extract_features(df)\n",
    "        print(f\"  新增特征数: {len(df_engineered.columns) - len(df.columns)}\")\n",
    "        \n",
    "        # 保存特征工程后的数据\n",
    "        df_engineered.to_excel(engineered_file, index=False)\n",
    "        print(f\"  已保存到: {engineered_file}\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  特征工程失败: {str(e)}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "print(\"\\n所有项目特征工程完成!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"特征工程后的数据集保存在: {engineered_dir}\")"
   ],
   "id": "37bb59a8b3446c67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始特征工程，共 10 个项目\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "特征工程:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理项目: django\n",
      "  输入文件: processed_dataset/cleaned\\django_cleaned.xlsx\n",
      "  原始记录数: 16976\n",
      "  原始列: ['segs_updated', 'avg_segs', 'merge_proportion_reviewer', 'comments', 'file_type', 'comment_length', 'author_num', 'closeness_centrality', 'eigenvector_centrality', 'state', 'name', 'betweenness_centrality', 'avg_churn_abandoned', 'avg_files_abandoned', 'modify_proportion', 'test_churn', 'name_reviewer', 'merged', 'changes_per_reviewer', 'total_additions', 'directory_num', 'title_length', 'is_author', 'segs_deleted', 'merge_proportion', 'avg_duration_reviewer', 'change_num', 'language_num', 'avg_round_reviewer', 'avg_duration', 'add_per_week', 'updated_at', 'participation_reviewer', 'title', 'commit_count', 'body_readability', 'avg_rounds_abandoned', 'degree_centrality', 'is_reviewer', 'avg_lines', 'body', 'last_comment_time', 'clustering_coefficient', 'project_age', 'experience_reviewer', 'avg_rounds', 'changed_files', 'avg_comments_abandoned', 'participation', 'betweenness_centrality_reviewer', 'files_deleted', 'closeness_centrality_reviewer', 'conversation', 'comment_num', 'total_deletions', 'merged_at', 'team_size', 'avg_duration_abandoned', 'segs_added', 'open_changes', 'files_updated', 'deletions', 'created_at', 'changes_per_week', 'avg_files', 'reviewer_num', 'last_comment_mention', 'degree_centrality_reviewer', 'avg_comments_merged', 'avg_files_merged', 'comment_embedding', 'bot_reviewer_num', 'non_test_churn', 'additions', 'is_reviewed', 'avg_churn_merged', 'avg_duration_merged', 'has_improve', 'k_coreness_reviewer', 'body_embedding', 'title_readability', 'del_per_week', 'total_files_changed', 'has_test', 'lines_deleted', 'experience', 'eigenvector_centrality_reviewer', 'avg_rounds_merged', 'avg_comment_length', 'changes_per_author', 'has_feature', 'body_length', 'change_num_reviewer', 'comment_count', 'commits', 'number', 'has_bug', 'has_document', 'title_embedding', 'closed_at', 'files_added', 'review_comments', 'k_coreness', 'avg_reviewers', 'has_refactor', 'modify_entropy', 'author', 'avg_round', 'clustering_coefficient_reviewer', 'avg_comments', 'lines_added']\n",
      "  处理时间列: created_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: closed_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: last_comment_time\n",
      "    成功转换为datetime类型\n",
      "  计算处理时间\n",
      "    添加特征: processing_time\n",
      "  计算最后响应时间\n",
      "    添加特征: last_response_time\n",
      "  分解创建时间\n",
      "    添加特征: created_hour, created_dayofweek, created_month\n",
      "  处理标题特征\n",
      "    添加特征: title_length\n",
      "  处理正文特征\n",
      "    添加特征: body_length\n",
      "  添加关键词特征\n",
      "    添加特征: has_bug_keyword\n",
      "    添加特征: has_feature_keyword\n",
      "    添加特征: has_document_keyword\n",
      "  处理变更特征\n",
      "    添加特征: total_changes, net_changes\n",
      "  处理文件变更特征\n",
      "    添加特征: change_density\n",
      "    添加特征: additions_per_file\n",
      "  处理作者特征\n",
      "    添加特征: author_experience\n",
      "    添加特征: author_activity\n",
      "  处理评审者特征\n",
      "    添加特征: reviewer_count\n",
      "  添加交互特征\n",
      "    添加特征: author_exp_change_size\n",
      "    添加特征: complexity_per_reviewer\n",
      "  新增特征数: 17\n",
      "  新增特征列表: ['created_month', 'total_changes', 'complexity_per_reviewer', 'reviewer_count', 'created_hour', 'last_response_time', 'additions_per_file', 'has_bug_keyword', 'processing_time', 'author_experience', 'change_density', 'has_feature_keyword', 'net_changes', 'author_activity', 'has_document_keyword', 'author_exp_change_size', 'created_dayofweek']\n",
      "  新增特征数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "特征工程:  10%|█         | 1/10 [01:04<09:44, 64.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/engineered\\django_engineered.xlsx\n",
      "--------------------------------------------------\n",
      "\n",
      "处理项目: moby\n",
      "  输入文件: processed_dataset/cleaned\\moby_cleaned.xlsx\n",
      "  原始记录数: 23515\n",
      "  原始列: ['segs_updated', 'avg_segs', 'merge_proportion_reviewer', 'comments', 'file_type', 'comment_length', 'author_num', 'closeness_centrality', 'eigenvector_centrality', 'state', 'name', 'betweenness_centrality', 'avg_churn_abandoned', 'avg_files_abandoned', 'modify_proportion', 'test_churn', 'name_reviewer', 'merged', 'changes_per_reviewer', 'total_additions', 'directory_num', 'title_length', 'is_author', 'segs_deleted', 'merge_proportion', 'avg_duration_reviewer', 'change_num', 'language_num', 'avg_round_reviewer', 'avg_duration', 'add_per_week', 'updated_at', 'participation_reviewer', 'title', 'commit_count', 'body_readability', 'avg_rounds_abandoned', 'degree_centrality', 'is_reviewer', 'avg_lines', 'body', 'last_comment_time', 'clustering_coefficient', 'project_age', 'experience_reviewer', 'avg_rounds', 'changed_files', 'avg_comments_abandoned', 'participation', 'betweenness_centrality_reviewer', 'files_deleted', 'closeness_centrality_reviewer', 'conversation', 'comment_num', 'total_deletions', 'merged_at', 'team_size', 'avg_duration_abandoned', 'segs_added', 'open_changes', 'files_updated', 'deletions', 'created_at', 'changes_per_week', 'avg_files', 'reviewer_num', 'last_comment_mention', 'degree_centrality_reviewer', 'avg_comments_merged', 'avg_files_merged', 'comment_embedding', 'bot_reviewer_num', 'non_test_churn', 'additions', 'is_reviewed', 'avg_churn_merged', 'avg_duration_merged', 'has_improve', 'k_coreness_reviewer', 'body_embedding', 'title_readability', 'del_per_week', 'total_files_changed', 'has_test', 'lines_deleted', 'experience', 'eigenvector_centrality_reviewer', 'avg_rounds_merged', 'avg_comment_length', 'changes_per_author', 'has_feature', 'body_length', 'change_num_reviewer', 'comment_count', 'commits', 'number', 'has_bug', 'has_document', 'title_embedding', 'closed_at', 'files_added', 'review_comments', 'k_coreness', 'avg_reviewers', 'has_refactor', 'modify_entropy', 'author', 'avg_round', 'clustering_coefficient_reviewer', 'avg_comments', 'lines_added']\n",
      "  处理时间列: created_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: closed_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: last_comment_time\n",
      "    成功转换为datetime类型\n",
      "  计算处理时间\n",
      "    添加特征: processing_time\n",
      "  计算最后响应时间\n",
      "    添加特征: last_response_time\n",
      "  分解创建时间\n",
      "    添加特征: created_hour, created_dayofweek, created_month\n",
      "  处理标题特征\n",
      "    添加特征: title_length\n",
      "  处理正文特征\n",
      "    添加特征: body_length\n",
      "  添加关键词特征\n",
      "    添加特征: has_bug_keyword\n",
      "    添加特征: has_feature_keyword\n",
      "    添加特征: has_document_keyword\n",
      "  处理变更特征\n",
      "    添加特征: total_changes, net_changes\n",
      "  处理文件变更特征\n",
      "    添加特征: change_density\n",
      "    添加特征: additions_per_file\n",
      "  处理作者特征\n",
      "    添加特征: author_experience\n",
      "    添加特征: author_activity\n",
      "  处理评审者特征\n",
      "    添加特征: reviewer_count\n",
      "  添加交互特征\n",
      "    添加特征: author_exp_change_size\n",
      "    添加特征: complexity_per_reviewer\n",
      "  新增特征数: 17\n",
      "  新增特征列表: ['created_month', 'total_changes', 'complexity_per_reviewer', 'reviewer_count', 'created_hour', 'last_response_time', 'additions_per_file', 'has_bug_keyword', 'processing_time', 'author_experience', 'change_density', 'has_feature_keyword', 'net_changes', 'author_activity', 'has_document_keyword', 'author_exp_change_size', 'created_dayofweek']\n",
      "  新增特征数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "特征工程:  20%|██        | 2/10 [03:20<14:10, 106.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/engineered\\moby_engineered.xlsx\n",
      "--------------------------------------------------\n",
      "\n",
      "处理项目: opencv\n",
      "  输入文件: processed_dataset/cleaned\\opencv_cleaned.xlsx\n",
      "  原始记录数: 13972\n",
      "  原始列: ['segs_updated', 'avg_segs', 'merge_proportion_reviewer', 'comments', 'file_type', 'comment_length', 'author_num', 'closeness_centrality', 'eigenvector_centrality', 'state', 'name', 'betweenness_centrality', 'avg_churn_abandoned', 'avg_files_abandoned', 'modify_proportion', 'test_churn', 'name_reviewer', 'merged', 'changes_per_reviewer', 'total_additions', 'directory_num', 'title_length', 'is_author', 'segs_deleted', 'merge_proportion', 'avg_duration_reviewer', 'change_num', 'language_num', 'avg_round_reviewer', 'avg_duration', 'add_per_week', 'updated_at', 'participation_reviewer', 'title', 'commit_count', 'body_readability', 'avg_rounds_abandoned', 'degree_centrality', 'is_reviewer', 'avg_lines', 'body', 'last_comment_time', 'clustering_coefficient', 'project_age', 'experience_reviewer', 'avg_rounds', 'changed_files', 'avg_comments_abandoned', 'participation', 'betweenness_centrality_reviewer', 'files_deleted', 'closeness_centrality_reviewer', 'conversation', 'comment_num', 'total_deletions', 'merged_at', 'team_size', 'avg_duration_abandoned', 'segs_added', 'open_changes', 'files_updated', 'deletions', 'created_at', 'changes_per_week', 'avg_files', 'reviewer_num', 'last_comment_mention', 'degree_centrality_reviewer', 'avg_comments_merged', 'avg_files_merged', 'comment_embedding', 'bot_reviewer_num', 'non_test_churn', 'additions', 'is_reviewed', 'avg_churn_merged', 'avg_duration_merged', 'has_improve', 'k_coreness_reviewer', 'body_embedding', 'title_readability', 'del_per_week', 'total_files_changed', 'has_test', 'lines_deleted', 'experience', 'eigenvector_centrality_reviewer', 'avg_rounds_merged', 'avg_comment_length', 'changes_per_author', 'has_feature', 'body_length', 'change_num_reviewer', 'comment_count', 'commits', 'number', 'has_bug', 'has_document', 'title_embedding', 'closed_at', 'files_added', 'review_comments', 'k_coreness', 'avg_reviewers', 'has_refactor', 'modify_entropy', 'author', 'avg_round', 'clustering_coefficient_reviewer', 'avg_comments', 'lines_added']\n",
      "  处理时间列: created_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: closed_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: last_comment_time\n",
      "    成功转换为datetime类型\n",
      "  计算处理时间\n",
      "    添加特征: processing_time\n",
      "  计算最后响应时间\n",
      "    添加特征: last_response_time\n",
      "  分解创建时间\n",
      "    添加特征: created_hour, created_dayofweek, created_month\n",
      "  处理标题特征\n",
      "    添加特征: title_length\n",
      "  处理正文特征\n",
      "    添加特征: body_length\n",
      "  添加关键词特征\n",
      "    添加特征: has_bug_keyword\n",
      "    添加特征: has_feature_keyword\n",
      "    添加特征: has_document_keyword\n",
      "  处理变更特征\n",
      "    添加特征: total_changes, net_changes\n",
      "  处理文件变更特征\n",
      "    添加特征: change_density\n",
      "    添加特征: additions_per_file\n",
      "  处理作者特征\n",
      "    添加特征: author_experience\n",
      "    添加特征: author_activity\n",
      "  处理评审者特征\n",
      "    添加特征: reviewer_count\n",
      "  添加交互特征\n",
      "    添加特征: author_exp_change_size\n",
      "    添加特征: complexity_per_reviewer\n",
      "  新增特征数: 17\n",
      "  新增特征列表: ['created_month', 'total_changes', 'complexity_per_reviewer', 'reviewer_count', 'created_hour', 'last_response_time', 'additions_per_file', 'has_bug_keyword', 'processing_time', 'author_experience', 'change_density', 'has_feature_keyword', 'net_changes', 'author_activity', 'has_document_keyword', 'author_exp_change_size', 'created_dayofweek']\n",
      "  新增特征数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "特征工程:  30%|███       | 3/10 [04:43<11:11, 95.99s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/engineered\\opencv_engineered.xlsx\n",
      "--------------------------------------------------\n",
      "\n",
      "处理项目: react\n",
      "  输入文件: processed_dataset/cleaned\\react_cleaned.xlsx\n",
      "  原始记录数: 13671\n",
      "  原始列: ['segs_updated', 'avg_segs', 'merge_proportion_reviewer', 'comments', 'file_type', 'comment_length', 'author_num', 'closeness_centrality', 'eigenvector_centrality', 'state', 'name', 'betweenness_centrality', 'avg_churn_abandoned', 'avg_files_abandoned', 'modify_proportion', 'test_churn', 'name_reviewer', 'merged', 'changes_per_reviewer', 'total_additions', 'directory_num', 'title_length', 'is_author', 'segs_deleted', 'merge_proportion', 'avg_duration_reviewer', 'change_num', 'language_num', 'avg_round_reviewer', 'avg_duration', 'add_per_week', 'updated_at', 'participation_reviewer', 'title', 'commit_count', 'body_readability', 'avg_rounds_abandoned', 'degree_centrality', 'is_reviewer', 'avg_lines', 'body', 'last_comment_time', 'clustering_coefficient', 'project_age', 'experience_reviewer', 'avg_rounds', 'changed_files', 'avg_comments_abandoned', 'participation', 'betweenness_centrality_reviewer', 'files_deleted', 'closeness_centrality_reviewer', 'conversation', 'comment_num', 'total_deletions', 'merged_at', 'team_size', 'avg_duration_abandoned', 'segs_added', 'open_changes', 'files_updated', 'deletions', 'created_at', 'changes_per_week', 'avg_files', 'reviewer_num', 'last_comment_mention', 'degree_centrality_reviewer', 'avg_comments_merged', 'avg_files_merged', 'comment_embedding', 'bot_reviewer_num', 'non_test_churn', 'additions', 'is_reviewed', 'avg_churn_merged', 'avg_duration_merged', 'has_improve', 'k_coreness_reviewer', 'body_embedding', 'title_readability', 'del_per_week', 'total_files_changed', 'has_test', 'lines_deleted', 'experience', 'eigenvector_centrality_reviewer', 'avg_rounds_merged', 'avg_comment_length', 'changes_per_author', 'has_feature', 'body_length', 'change_num_reviewer', 'comment_count', 'commits', 'number', 'has_bug', 'has_document', 'title_embedding', 'closed_at', 'files_added', 'review_comments', 'k_coreness', 'avg_reviewers', 'has_refactor', 'modify_entropy', 'author', 'avg_round', 'clustering_coefficient_reviewer', 'avg_comments', 'lines_added']\n",
      "  处理时间列: created_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: closed_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: last_comment_time\n",
      "    成功转换为datetime类型\n",
      "  计算处理时间\n",
      "    添加特征: processing_time\n",
      "  计算最后响应时间\n",
      "    添加特征: last_response_time\n",
      "  分解创建时间\n",
      "    添加特征: created_hour, created_dayofweek, created_month\n",
      "  处理标题特征\n",
      "    添加特征: title_length\n",
      "  处理正文特征\n",
      "    添加特征: body_length\n",
      "  添加关键词特征\n",
      "    添加特征: has_bug_keyword\n",
      "    添加特征: has_feature_keyword\n",
      "    添加特征: has_document_keyword\n",
      "  处理变更特征\n",
      "    添加特征: total_changes, net_changes\n",
      "  处理文件变更特征\n",
      "    添加特征: change_density\n",
      "    添加特征: additions_per_file\n",
      "  处理作者特征\n",
      "    添加特征: author_experience\n",
      "    添加特征: author_activity\n",
      "  处理评审者特征\n",
      "    添加特征: reviewer_count\n",
      "  添加交互特征\n",
      "    添加特征: author_exp_change_size\n",
      "    添加特征: complexity_per_reviewer\n",
      "  新增特征数: 17\n",
      "  新增特征列表: ['created_month', 'total_changes', 'complexity_per_reviewer', 'reviewer_count', 'created_hour', 'last_response_time', 'additions_per_file', 'has_bug_keyword', 'processing_time', 'author_experience', 'change_density', 'has_feature_keyword', 'net_changes', 'author_activity', 'has_document_keyword', 'author_exp_change_size', 'created_dayofweek']\n",
      "  新增特征数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "特征工程:  40%|████      | 4/10 [06:04<08:59, 89.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/engineered\\react_engineered.xlsx\n",
      "--------------------------------------------------\n",
      "\n",
      "处理项目: salt\n",
      "  输入文件: processed_dataset/cleaned\\salt_cleaned.xlsx\n",
      "  原始记录数: 39025\n",
      "  原始列: ['segs_updated', 'avg_segs', 'merge_proportion_reviewer', 'comments', 'file_type', 'comment_length', 'author_num', 'closeness_centrality', 'eigenvector_centrality', 'state', 'name', 'betweenness_centrality', 'avg_churn_abandoned', 'avg_files_abandoned', 'modify_proportion', 'test_churn', 'name_reviewer', 'merged', 'changes_per_reviewer', 'total_additions', 'directory_num', 'title_length', 'is_author', 'segs_deleted', 'merge_proportion', 'avg_duration_reviewer', 'change_num', 'language_num', 'avg_round_reviewer', 'avg_duration', 'add_per_week', 'updated_at', 'participation_reviewer', 'title', 'commit_count', 'body_readability', 'avg_rounds_abandoned', 'degree_centrality', 'is_reviewer', 'avg_lines', 'body', 'last_comment_time', 'clustering_coefficient', 'project_age', 'experience_reviewer', 'avg_rounds', 'changed_files', 'avg_comments_abandoned', 'participation', 'betweenness_centrality_reviewer', 'files_deleted', 'closeness_centrality_reviewer', 'conversation', 'comment_num', 'total_deletions', 'merged_at', 'team_size', 'avg_duration_abandoned', 'segs_added', 'open_changes', 'files_updated', 'deletions', 'created_at', 'changes_per_week', 'avg_files', 'reviewer_num', 'last_comment_mention', 'degree_centrality_reviewer', 'avg_comments_merged', 'avg_files_merged', 'comment_embedding', 'bot_reviewer_num', 'non_test_churn', 'additions', 'is_reviewed', 'avg_churn_merged', 'avg_duration_merged', 'has_improve', 'k_coreness_reviewer', 'body_embedding', 'title_readability', 'del_per_week', 'total_files_changed', 'has_test', 'lines_deleted', 'experience', 'eigenvector_centrality_reviewer', 'avg_rounds_merged', 'avg_comment_length', 'changes_per_author', 'has_feature', 'body_length', 'change_num_reviewer', 'comment_count', 'commits', 'number', 'has_bug', 'has_document', 'title_embedding', 'closed_at', 'files_added', 'review_comments', 'k_coreness', 'avg_reviewers', 'has_refactor', 'modify_entropy', 'author', 'avg_round', 'clustering_coefficient_reviewer', 'avg_comments', 'lines_added']\n",
      "  处理时间列: created_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: closed_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: last_comment_time\n",
      "    成功转换为datetime类型\n",
      "  计算处理时间\n",
      "    添加特征: processing_time\n",
      "  计算最后响应时间\n",
      "    添加特征: last_response_time\n",
      "  分解创建时间\n",
      "    添加特征: created_hour, created_dayofweek, created_month\n",
      "  处理标题特征\n",
      "    添加特征: title_length\n",
      "  处理正文特征\n",
      "    添加特征: body_length\n",
      "  添加关键词特征\n",
      "    添加特征: has_bug_keyword\n",
      "    添加特征: has_feature_keyword\n",
      "    添加特征: has_document_keyword\n",
      "  处理变更特征\n",
      "    添加特征: total_changes, net_changes\n",
      "  处理文件变更特征\n",
      "    添加特征: change_density\n",
      "    添加特征: additions_per_file\n",
      "  处理作者特征\n",
      "    添加特征: author_experience\n",
      "    添加特征: author_activity\n",
      "  处理评审者特征\n",
      "    添加特征: reviewer_count\n",
      "  添加交互特征\n",
      "    添加特征: author_exp_change_size\n",
      "    添加特征: complexity_per_reviewer\n",
      "  新增特征数: 17\n",
      "  新增特征列表: ['created_month', 'total_changes', 'complexity_per_reviewer', 'reviewer_count', 'created_hour', 'last_response_time', 'additions_per_file', 'has_bug_keyword', 'processing_time', 'author_experience', 'change_density', 'has_feature_keyword', 'net_changes', 'author_activity', 'has_document_keyword', 'author_exp_change_size', 'created_dayofweek']\n",
      "  新增特征数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "特征工程:  50%|█████     | 5/10 [08:49<09:44, 116.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/engineered\\salt_engineered.xlsx\n",
      "--------------------------------------------------\n",
      "\n",
      "处理项目: scikit-learn\n",
      "  输入文件: processed_dataset/cleaned\\scikit-learn_cleaned.xlsx\n",
      "  原始记录数: 15687\n",
      "  原始列: ['segs_updated', 'avg_segs', 'merge_proportion_reviewer', 'comments', 'file_type', 'comment_length', 'author_num', 'closeness_centrality', 'eigenvector_centrality', 'state', 'name', 'betweenness_centrality', 'avg_churn_abandoned', 'avg_files_abandoned', 'modify_proportion', 'test_churn', 'name_reviewer', 'merged', 'changes_per_reviewer', 'total_additions', 'directory_num', 'title_length', 'is_author', 'segs_deleted', 'merge_proportion', 'avg_duration_reviewer', 'change_num', 'language_num', 'avg_round_reviewer', 'avg_duration', 'add_per_week', 'updated_at', 'participation_reviewer', 'title', 'commit_count', 'body_readability', 'avg_rounds_abandoned', 'degree_centrality', 'is_reviewer', 'avg_lines', 'body', 'last_comment_time', 'clustering_coefficient', 'project_age', 'experience_reviewer', 'avg_rounds', 'changed_files', 'avg_comments_abandoned', 'participation', 'betweenness_centrality_reviewer', 'files_deleted', 'closeness_centrality_reviewer', 'conversation', 'comment_num', 'total_deletions', 'merged_at', 'team_size', 'avg_duration_abandoned', 'segs_added', 'open_changes', 'files_updated', 'deletions', 'created_at', 'changes_per_week', 'avg_files', 'reviewer_num', 'last_comment_mention', 'degree_centrality_reviewer', 'avg_comments_merged', 'avg_files_merged', 'comment_embedding', 'bot_reviewer_num', 'non_test_churn', 'additions', 'is_reviewed', 'avg_churn_merged', 'avg_duration_merged', 'has_improve', 'k_coreness_reviewer', 'body_embedding', 'title_readability', 'del_per_week', 'total_files_changed', 'has_test', 'lines_deleted', 'experience', 'eigenvector_centrality_reviewer', 'avg_rounds_merged', 'avg_comment_length', 'changes_per_author', 'has_feature', 'body_length', 'change_num_reviewer', 'comment_count', 'commits', 'number', 'has_bug', 'has_document', 'title_embedding', 'closed_at', 'files_added', 'review_comments', 'k_coreness', 'avg_reviewers', 'has_refactor', 'modify_entropy', 'author', 'avg_round', 'clustering_coefficient_reviewer', 'avg_comments', 'lines_added']\n",
      "  处理时间列: created_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: closed_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: last_comment_time\n",
      "    成功转换为datetime类型\n",
      "  计算处理时间\n",
      "    添加特征: processing_time\n",
      "  计算最后响应时间\n",
      "    添加特征: last_response_time\n",
      "  分解创建时间\n",
      "    添加特征: created_hour, created_dayofweek, created_month\n",
      "  处理标题特征\n",
      "    添加特征: title_length\n",
      "  处理正文特征\n",
      "    添加特征: body_length\n",
      "  添加关键词特征\n",
      "    添加特征: has_bug_keyword\n",
      "    添加特征: has_feature_keyword\n",
      "    添加特征: has_document_keyword\n",
      "  处理变更特征\n",
      "    添加特征: total_changes, net_changes\n",
      "  处理文件变更特征\n",
      "    添加特征: change_density\n",
      "    添加特征: additions_per_file\n",
      "  处理作者特征\n",
      "    添加特征: author_experience\n",
      "    添加特征: author_activity\n",
      "  处理评审者特征\n",
      "    添加特征: reviewer_count\n",
      "  添加交互特征\n",
      "    添加特征: author_exp_change_size\n",
      "    添加特征: complexity_per_reviewer\n",
      "  新增特征数: 17\n",
      "  新增特征列表: ['created_month', 'total_changes', 'complexity_per_reviewer', 'reviewer_count', 'created_hour', 'last_response_time', 'additions_per_file', 'has_bug_keyword', 'processing_time', 'author_experience', 'change_density', 'has_feature_keyword', 'net_changes', 'author_activity', 'has_document_keyword', 'author_exp_change_size', 'created_dayofweek']\n",
      "  新增特征数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "特征工程:  60%|██████    | 6/10 [09:51<06:33, 98.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/engineered\\scikit-learn_engineered.xlsx\n",
      "--------------------------------------------------\n",
      "\n",
      "处理项目: symfony\n",
      "  输入文件: processed_dataset/cleaned\\symfony_cleaned.xlsx\n",
      "  原始记录数: 30392\n",
      "  原始列: ['segs_updated', 'avg_segs', 'merge_proportion_reviewer', 'comments', 'file_type', 'comment_length', 'author_num', 'closeness_centrality', 'eigenvector_centrality', 'state', 'name', 'betweenness_centrality', 'avg_churn_abandoned', 'avg_files_abandoned', 'modify_proportion', 'test_churn', 'name_reviewer', 'merged', 'changes_per_reviewer', 'total_additions', 'directory_num', 'title_length', 'is_author', 'segs_deleted', 'merge_proportion', 'avg_duration_reviewer', 'change_num', 'language_num', 'avg_round_reviewer', 'avg_duration', 'add_per_week', 'updated_at', 'participation_reviewer', 'title', 'commit_count', 'body_readability', 'avg_rounds_abandoned', 'degree_centrality', 'is_reviewer', 'avg_lines', 'body', 'last_comment_time', 'clustering_coefficient', 'project_age', 'experience_reviewer', 'avg_rounds', 'changed_files', 'avg_comments_abandoned', 'participation', 'betweenness_centrality_reviewer', 'files_deleted', 'closeness_centrality_reviewer', 'conversation', 'comment_num', 'total_deletions', 'merged_at', 'team_size', 'avg_duration_abandoned', 'segs_added', 'open_changes', 'files_updated', 'deletions', 'created_at', 'changes_per_week', 'avg_files', 'reviewer_num', 'last_comment_mention', 'degree_centrality_reviewer', 'avg_comments_merged', 'avg_files_merged', 'comment_embedding', 'bot_reviewer_num', 'non_test_churn', 'additions', 'is_reviewed', 'avg_churn_merged', 'avg_duration_merged', 'has_improve', 'k_coreness_reviewer', 'body_embedding', 'title_readability', 'del_per_week', 'total_files_changed', 'has_test', 'lines_deleted', 'experience', 'eigenvector_centrality_reviewer', 'avg_rounds_merged', 'avg_comment_length', 'changes_per_author', 'has_feature', 'body_length', 'change_num_reviewer', 'comment_count', 'commits', 'number', 'has_bug', 'has_document', 'title_embedding', 'closed_at', 'files_added', 'review_comments', 'k_coreness', 'avg_reviewers', 'has_refactor', 'modify_entropy', 'author', 'avg_round', 'clustering_coefficient_reviewer', 'avg_comments', 'lines_added']\n",
      "  处理时间列: created_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: closed_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: last_comment_time\n",
      "    成功转换为datetime类型\n",
      "  计算处理时间\n",
      "    添加特征: processing_time\n",
      "  计算最后响应时间\n",
      "    添加特征: last_response_time\n",
      "  分解创建时间\n",
      "    添加特征: created_hour, created_dayofweek, created_month\n",
      "  处理标题特征\n",
      "    添加特征: title_length\n",
      "  处理正文特征\n",
      "    添加特征: body_length\n",
      "  添加关键词特征\n",
      "    添加特征: has_bug_keyword\n",
      "    添加特征: has_feature_keyword\n",
      "    添加特征: has_document_keyword\n",
      "  处理变更特征\n",
      "    添加特征: total_changes, net_changes\n",
      "  处理文件变更特征\n",
      "    添加特征: change_density\n",
      "    添加特征: additions_per_file\n",
      "  处理作者特征\n",
      "    添加特征: author_experience\n",
      "    添加特征: author_activity\n",
      "  处理评审者特征\n",
      "    添加特征: reviewer_count\n",
      "  添加交互特征\n",
      "    添加特征: author_exp_change_size\n",
      "    添加特征: complexity_per_reviewer\n",
      "  新增特征数: 17\n",
      "  新增特征列表: ['created_month', 'total_changes', 'complexity_per_reviewer', 'reviewer_count', 'created_hour', 'last_response_time', 'additions_per_file', 'has_bug_keyword', 'processing_time', 'author_experience', 'change_density', 'has_feature_keyword', 'net_changes', 'author_activity', 'has_document_keyword', 'author_exp_change_size', 'created_dayofweek']\n",
      "  新增特征数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "特征工程:  70%|███████   | 7/10 [11:48<05:13, 104.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/engineered\\symfony_engineered.xlsx\n",
      "--------------------------------------------------\n",
      "\n",
      "处理项目: tensorflow\n",
      "  输入文件: processed_dataset/cleaned\\tensorflow_cleaned.xlsx\n",
      "  原始记录数: 23130\n",
      "  原始列: ['segs_updated', 'avg_segs', 'merge_proportion_reviewer', 'comments', 'file_type', 'comment_length', 'author_num', 'closeness_centrality', 'eigenvector_centrality', 'state', 'name', 'betweenness_centrality', 'avg_churn_abandoned', 'avg_files_abandoned', 'modify_proportion', 'test_churn', 'name_reviewer', 'merged', 'changes_per_reviewer', 'total_additions', 'directory_num', 'title_length', 'is_author', 'segs_deleted', 'merge_proportion', 'avg_duration_reviewer', 'change_num', 'language_num', 'avg_round_reviewer', 'avg_duration', 'add_per_week', 'updated_at', 'participation_reviewer', 'title', 'commit_count', 'body_readability', 'avg_rounds_abandoned', 'degree_centrality', 'is_reviewer', 'avg_lines', 'body', 'last_comment_time', 'clustering_coefficient', 'project_age', 'experience_reviewer', 'avg_rounds', 'changed_files', 'avg_comments_abandoned', 'participation', 'betweenness_centrality_reviewer', 'files_deleted', 'closeness_centrality_reviewer', 'conversation', 'comment_num', 'total_deletions', 'merged_at', 'team_size', 'avg_duration_abandoned', 'segs_added', 'open_changes', 'files_updated', 'deletions', 'created_at', 'changes_per_week', 'avg_files', 'reviewer_num', 'last_comment_mention', 'degree_centrality_reviewer', 'avg_comments_merged', 'avg_files_merged', 'comment_embedding', 'bot_reviewer_num', 'non_test_churn', 'additions', 'is_reviewed', 'avg_churn_merged', 'avg_duration_merged', 'has_improve', 'k_coreness_reviewer', 'body_embedding', 'title_readability', 'del_per_week', 'total_files_changed', 'has_test', 'lines_deleted', 'experience', 'eigenvector_centrality_reviewer', 'avg_rounds_merged', 'avg_comment_length', 'changes_per_author', 'has_feature', 'body_length', 'change_num_reviewer', 'comment_count', 'commits', 'number', 'has_bug', 'has_document', 'title_embedding', 'closed_at', 'files_added', 'review_comments', 'k_coreness', 'avg_reviewers', 'has_refactor', 'modify_entropy', 'author', 'avg_round', 'clustering_coefficient_reviewer', 'avg_comments', 'lines_added']\n",
      "  处理时间列: created_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: closed_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: last_comment_time\n",
      "    成功转换为datetime类型\n",
      "  计算处理时间\n",
      "    添加特征: processing_time\n",
      "  计算最后响应时间\n",
      "    添加特征: last_response_time\n",
      "  分解创建时间\n",
      "    添加特征: created_hour, created_dayofweek, created_month\n",
      "  处理标题特征\n",
      "    添加特征: title_length\n",
      "  处理正文特征\n",
      "    添加特征: body_length\n",
      "  添加关键词特征\n",
      "    添加特征: has_bug_keyword\n",
      "    添加特征: has_feature_keyword\n",
      "    添加特征: has_document_keyword\n",
      "  处理变更特征\n",
      "    添加特征: total_changes, net_changes\n",
      "  处理文件变更特征\n",
      "    添加特征: change_density\n",
      "    添加特征: additions_per_file\n",
      "  处理作者特征\n",
      "    添加特征: author_experience\n",
      "    添加特征: author_activity\n",
      "  处理评审者特征\n",
      "    添加特征: reviewer_count\n",
      "  添加交互特征\n",
      "    添加特征: author_exp_change_size\n",
      "    添加特征: complexity_per_reviewer\n",
      "  新增特征数: 17\n",
      "  新增特征列表: ['created_month', 'total_changes', 'complexity_per_reviewer', 'reviewer_count', 'created_hour', 'last_response_time', 'additions_per_file', 'has_bug_keyword', 'processing_time', 'author_experience', 'change_density', 'has_feature_keyword', 'net_changes', 'author_activity', 'has_document_keyword', 'author_exp_change_size', 'created_dayofweek']\n",
      "  新增特征数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "特征工程:  80%|████████  | 8/10 [13:15<03:17, 99.00s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/engineered\\tensorflow_engineered.xlsx\n",
      "--------------------------------------------------\n",
      "\n",
      "处理项目: terraform\n",
      "  输入文件: processed_dataset/cleaned\\terraform_cleaned.xlsx\n",
      "  原始记录数: 13219\n",
      "  原始列: ['segs_updated', 'avg_segs', 'merge_proportion_reviewer', 'comments', 'file_type', 'comment_length', 'author_num', 'closeness_centrality', 'eigenvector_centrality', 'state', 'name', 'betweenness_centrality', 'avg_churn_abandoned', 'avg_files_abandoned', 'modify_proportion', 'test_churn', 'name_reviewer', 'merged', 'changes_per_reviewer', 'total_additions', 'directory_num', 'title_length', 'is_author', 'segs_deleted', 'merge_proportion', 'avg_duration_reviewer', 'change_num', 'language_num', 'avg_round_reviewer', 'avg_duration', 'add_per_week', 'updated_at', 'participation_reviewer', 'title', 'commit_count', 'body_readability', 'avg_rounds_abandoned', 'degree_centrality', 'is_reviewer', 'avg_lines', 'body', 'last_comment_time', 'clustering_coefficient', 'project_age', 'experience_reviewer', 'avg_rounds', 'changed_files', 'avg_comments_abandoned', 'participation', 'betweenness_centrality_reviewer', 'files_deleted', 'closeness_centrality_reviewer', 'conversation', 'comment_num', 'total_deletions', 'merged_at', 'team_size', 'avg_duration_abandoned', 'segs_added', 'open_changes', 'files_updated', 'deletions', 'created_at', 'changes_per_week', 'avg_files', 'reviewer_num', 'last_comment_mention', 'degree_centrality_reviewer', 'avg_comments_merged', 'avg_files_merged', 'comment_embedding', 'bot_reviewer_num', 'non_test_churn', 'additions', 'is_reviewed', 'avg_churn_merged', 'avg_duration_merged', 'has_improve', 'k_coreness_reviewer', 'body_embedding', 'title_readability', 'del_per_week', 'total_files_changed', 'has_test', 'lines_deleted', 'experience', 'eigenvector_centrality_reviewer', 'avg_rounds_merged', 'avg_comment_length', 'changes_per_author', 'has_feature', 'body_length', 'change_num_reviewer', 'comment_count', 'commits', 'number', 'has_bug', 'has_document', 'title_embedding', 'closed_at', 'files_added', 'review_comments', 'k_coreness', 'avg_reviewers', 'has_refactor', 'modify_entropy', 'author', 'avg_round', 'clustering_coefficient_reviewer', 'avg_comments', 'lines_added']\n",
      "  处理时间列: created_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: closed_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: last_comment_time\n",
      "    成功转换为datetime类型\n",
      "  计算处理时间\n",
      "    添加特征: processing_time\n",
      "  计算最后响应时间\n",
      "    添加特征: last_response_time\n",
      "  分解创建时间\n",
      "    添加特征: created_hour, created_dayofweek, created_month\n",
      "  处理标题特征\n",
      "    添加特征: title_length\n",
      "  处理正文特征\n",
      "    添加特征: body_length\n",
      "  添加关键词特征\n",
      "    添加特征: has_bug_keyword\n",
      "    添加特征: has_feature_keyword\n",
      "    添加特征: has_document_keyword\n",
      "  处理变更特征\n",
      "    添加特征: total_changes, net_changes\n",
      "  处理文件变更特征\n",
      "    添加特征: change_density\n",
      "    添加特征: additions_per_file\n",
      "  处理作者特征\n",
      "    添加特征: author_experience\n",
      "    添加特征: author_activity\n",
      "  处理评审者特征\n",
      "    添加特征: reviewer_count\n",
      "  添加交互特征\n",
      "    添加特征: author_exp_change_size\n",
      "    添加特征: complexity_per_reviewer\n",
      "  新增特征数: 17\n",
      "  新增特征列表: ['created_month', 'total_changes', 'complexity_per_reviewer', 'reviewer_count', 'created_hour', 'last_response_time', 'additions_per_file', 'has_bug_keyword', 'processing_time', 'author_experience', 'change_density', 'has_feature_keyword', 'net_changes', 'author_activity', 'has_document_keyword', 'author_exp_change_size', 'created_dayofweek']\n",
      "  新增特征数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "特征工程:  90%|█████████ | 9/10 [14:06<01:24, 84.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/engineered\\terraform_engineered.xlsx\n",
      "--------------------------------------------------\n",
      "\n",
      "处理项目: yii2\n",
      "  输入文件: processed_dataset/cleaned\\yii2_cleaned.xlsx\n",
      "  原始记录数: 8040\n",
      "  原始列: ['segs_updated', 'avg_segs', 'merge_proportion_reviewer', 'comments', 'file_type', 'comment_length', 'author_num', 'closeness_centrality', 'eigenvector_centrality', 'state', 'name', 'betweenness_centrality', 'avg_churn_abandoned', 'avg_files_abandoned', 'modify_proportion', 'test_churn', 'name_reviewer', 'merged', 'changes_per_reviewer', 'total_additions', 'directory_num', 'title_length', 'is_author', 'segs_deleted', 'merge_proportion', 'avg_duration_reviewer', 'change_num', 'language_num', 'avg_round_reviewer', 'avg_duration', 'add_per_week', 'updated_at', 'participation_reviewer', 'title', 'commit_count', 'body_readability', 'avg_rounds_abandoned', 'degree_centrality', 'is_reviewer', 'avg_lines', 'body', 'last_comment_time', 'clustering_coefficient', 'project_age', 'experience_reviewer', 'avg_rounds', 'changed_files', 'avg_comments_abandoned', 'participation', 'betweenness_centrality_reviewer', 'files_deleted', 'closeness_centrality_reviewer', 'conversation', 'comment_num', 'total_deletions', 'merged_at', 'team_size', 'avg_duration_abandoned', 'segs_added', 'open_changes', 'files_updated', 'deletions', 'created_at', 'changes_per_week', 'avg_files', 'reviewer_num', 'last_comment_mention', 'degree_centrality_reviewer', 'avg_comments_merged', 'avg_files_merged', 'comment_embedding', 'bot_reviewer_num', 'non_test_churn', 'additions', 'is_reviewed', 'avg_churn_merged', 'avg_duration_merged', 'has_improve', 'k_coreness_reviewer', 'body_embedding', 'title_readability', 'del_per_week', 'total_files_changed', 'has_test', 'lines_deleted', 'experience', 'eigenvector_centrality_reviewer', 'avg_rounds_merged', 'avg_comment_length', 'changes_per_author', 'has_feature', 'body_length', 'change_num_reviewer', 'comment_count', 'commits', 'number', 'has_bug', 'has_document', 'title_embedding', 'closed_at', 'files_added', 'review_comments', 'k_coreness', 'avg_reviewers', 'has_refactor', 'modify_entropy', 'author', 'avg_round', 'clustering_coefficient_reviewer', 'avg_comments', 'lines_added']\n",
      "  处理时间列: created_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: closed_at\n",
      "    成功转换为datetime类型\n",
      "  处理时间列: last_comment_time\n",
      "    成功转换为datetime类型\n",
      "  计算处理时间\n",
      "    添加特征: processing_time\n",
      "  计算最后响应时间\n",
      "    添加特征: last_response_time\n",
      "  分解创建时间\n",
      "    添加特征: created_hour, created_dayofweek, created_month\n",
      "  处理标题特征\n",
      "    添加特征: title_length\n",
      "  处理正文特征\n",
      "    添加特征: body_length\n",
      "  添加关键词特征\n",
      "    添加特征: has_bug_keyword\n",
      "    添加特征: has_feature_keyword\n",
      "    添加特征: has_document_keyword\n",
      "  处理变更特征\n",
      "    添加特征: total_changes, net_changes\n",
      "  处理文件变更特征\n",
      "    添加特征: change_density\n",
      "    添加特征: additions_per_file\n",
      "  处理作者特征\n",
      "    添加特征: author_experience\n",
      "    添加特征: author_activity\n",
      "  处理评审者特征\n",
      "    添加特征: reviewer_count\n",
      "  添加交互特征\n",
      "    添加特征: author_exp_change_size\n",
      "    添加特征: complexity_per_reviewer\n",
      "  新增特征数: 17\n",
      "  新增特征列表: ['created_month', 'total_changes', 'complexity_per_reviewer', 'reviewer_count', 'created_hour', 'last_response_time', 'additions_per_file', 'has_bug_keyword', 'processing_time', 'author_experience', 'change_density', 'has_feature_keyword', 'net_changes', 'author_activity', 'has_document_keyword', 'author_exp_change_size', 'created_dayofweek']\n",
      "  新增特征数: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "特征工程: 100%|██████████| 10/10 [14:37<00:00, 87.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  已保存到: processed_dataset/engineered\\yii2_engineered.xlsx\n",
      "--------------------------------------------------\n",
      "\n",
      "所有项目特征工程完成!\n",
      "==================================================\n",
      "特征工程后的数据集保存在: processed_dataset/engineered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fe24be9c8cb2bf8b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
